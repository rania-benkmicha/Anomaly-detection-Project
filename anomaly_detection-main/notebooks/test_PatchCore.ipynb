{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "huWHlGv6TJ6o",
        "sp2Hainrt1BS",
        "R3PyNU5FtxCa",
        "fA0dJ1TItqmR",
        "GSx0hurbtjJo",
        "JEJreVA4te-Y",
        "rxcVTKf_tcP3",
        "u0VawNbptWp_",
        "gcNDFx-ru0uZ",
        "E5fPncRyvz0n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements"
      ],
      "metadata": {
        "id": "huWHlGv6TJ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install click>=8.0.3 faiss-cpu matplotlib>=3.5.0 pillow>=8.4.0 pretrainedmodels>=0.7.4 torch>=1.10.0 scikit-image>=0.18.3 scikit-learn>=1.0.1 scipy>=1.7.1 torchvision>=0.11.1 tqdm>=4.62.3 timm"
      ],
      "metadata": {
        "id": "siCANOgpoq0S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset mvtec"
      ],
      "metadata": {
        "id": "sp2Hainrt1BS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d8smcA7xmONg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from enum import Enum\n",
        "\n",
        "import PIL\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "_CLASSNAMES = [\n",
        "    \"bottle\",\n",
        "    \"cable\",\n",
        "    \"capsule\",\n",
        "    \"carpet\",\n",
        "    \"grid\",\n",
        "    \"hazelnut\",\n",
        "    \"leather\",\n",
        "    \"metal_nut\",\n",
        "    \"pill\",\n",
        "    \"screw\",\n",
        "    \"tile\",\n",
        "    \"toothbrush\",\n",
        "    \"transistor\",\n",
        "    \"wood\",\n",
        "    \"zipper\",\n",
        "]\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "class DatasetSplit(Enum):\n",
        "    TRAIN = \"train\"\n",
        "    VAL = \"val\"\n",
        "    TEST = \"test\"\n",
        "\n",
        "\n",
        "class MVTecDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for MVTec.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        source,\n",
        "        classname,\n",
        "        resize=256,\n",
        "        imagesize=224,\n",
        "        split=DatasetSplit.TRAIN,\n",
        "        train_val_split=1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            source: [str]. Path to the MVTec data folder.\n",
        "            classname: [str or None]. Name of MVTec class that should be\n",
        "                       provided in this dataset. If None, the datasets\n",
        "                       iterates over all available images.\n",
        "            resize: [int]. (Square) Size the loaded image initially gets\n",
        "                    resized to.\n",
        "            imagesize: [int]. (Square) Size the resized loaded image gets\n",
        "                       (center-)cropped to.\n",
        "            split: [enum-option]. Indicates if training or test split of the\n",
        "                   data should be used. Has to be an option taken from\n",
        "                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n",
        "                   mvtec.DatasetSplit.TEST will also load mask data.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.source = source\n",
        "        self.split = split\n",
        "        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n",
        "        self.train_val_split = train_val_split\n",
        "\n",
        "        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n",
        "\n",
        "        self.transform_img = [\n",
        "            transforms.Resize(resize),\n",
        "            transforms.CenterCrop(imagesize),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ]\n",
        "        self.transform_img = transforms.Compose(self.transform_img)\n",
        "\n",
        "        self.transform_mask = [\n",
        "            transforms.Resize(resize),\n",
        "            transforms.CenterCrop(imagesize),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "        self.transform_mask = transforms.Compose(self.transform_mask)\n",
        "\n",
        "        self.imagesize = (3, imagesize, imagesize)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform_img(image)\n",
        "\n",
        "        if self.split == DatasetSplit.TEST and mask_path is not None:\n",
        "            mask = PIL.Image.open(mask_path)\n",
        "            mask = self.transform_mask(mask)\n",
        "        else:\n",
        "            mask = torch.zeros([1, *image.size()[1:]])\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"mask\": mask,\n",
        "            \"classname\": classname,\n",
        "            \"anomaly\": anomaly,\n",
        "            \"is_anomaly\": int(anomaly != \"good\"),\n",
        "            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n",
        "            \"image_path\": image_path,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_to_iterate)\n",
        "\n",
        "    def get_image_data(self):\n",
        "        imgpaths_per_class = {}\n",
        "        maskpaths_per_class = {}\n",
        "        \n",
        "        for classname in self.classnames_to_use:\n",
        "            classpath = os.path.join(self.source, classname, self.split.value)\n",
        "            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n",
        "            anomaly_types = os.listdir(classpath)\n",
        "\n",
        "            imgpaths_per_class[classname] = {}\n",
        "            maskpaths_per_class[classname] = {}\n",
        "\n",
        "            for anomaly in anomaly_types:\n",
        "                anomaly_path = os.path.join(classpath, anomaly)\n",
        "                anomaly_files = sorted(os.listdir(anomaly_path))\n",
        "                imgpaths_per_class[classname][anomaly] = [\n",
        "                    os.path.join(anomaly_path, x) for x in anomaly_files\n",
        "                ]\n",
        "\n",
        "                if self.train_val_split < 1.0:\n",
        "                    n_images = len(imgpaths_per_class[classname][anomaly])\n",
        "                    train_val_split_idx = int(n_images * self.train_val_split)\n",
        "                    if self.split == DatasetSplit.TRAIN:\n",
        "                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n",
        "                            classname\n",
        "                        ][anomaly][:train_val_split_idx]\n",
        "                    elif self.split == DatasetSplit.VAL:\n",
        "                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n",
        "                            classname\n",
        "                        ][anomaly][train_val_split_idx:]\n",
        "\n",
        "                if self.split == DatasetSplit.TEST and anomaly != \"good\":\n",
        "                    anomaly_mask_path = os.path.join(maskpath, anomaly)\n",
        "                    anomaly_mask_files = sorted(os.listdir(anomaly_mask_path))\n",
        "                    maskpaths_per_class[classname][anomaly] = [\n",
        "                        os.path.join(anomaly_mask_path, x) for x in anomaly_mask_files\n",
        "                    ]\n",
        "                else:\n",
        "                    maskpaths_per_class[classname][\"good\"] = None\n",
        "\n",
        "        # Unrolls the data dictionary to an easy-to-iterate list.\n",
        "        data_to_iterate = []\n",
        "        for classname in sorted(imgpaths_per_class.keys()):\n",
        "            for anomaly in sorted(imgpaths_per_class[classname].keys()):\n",
        "                for i, image_path in enumerate(imgpaths_per_class[classname][anomaly]):\n",
        "                    data_tuple = [classname, anomaly, image_path]\n",
        "                    if self.split == DatasetSplit.TEST and anomaly != \"good\":\n",
        "                        data_tuple.append(maskpaths_per_class[classname][anomaly][i])\n",
        "                    else:\n",
        "                        data_tuple.append(None)\n",
        "                    data_to_iterate.append(data_tuple)\n",
        "\n",
        "        return imgpaths_per_class, data_to_iterate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#backbones"
      ],
      "metadata": {
        "id": "R3PyNU5FtxCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm  # noqa\n",
        "import torchvision.models as models  # noqa\n",
        "\n",
        "_BACKBONES = {\n",
        "    \"alexnet\": \"models.alexnet(pretrained=True)\",\n",
        "    \"bninception\": 'pretrainedmodels.__dict__[\"bninception\"]'\n",
        "    '(pretrained=\"imagenet\", num_classes=1000)',\n",
        "    \"resnet50\": \"models.resnet50(pretrained=True)\",\n",
        "    \"resnet101\": \"models.resnet101(pretrained=True)\",\n",
        "    \"resnext101\": \"models.resnext101_32x8d(pretrained=True)\",\n",
        "    \"resnet200\": 'timm.create_model(\"resnet200\", pretrained=True)',\n",
        "    \"resnest50\": 'timm.create_model(\"resnest50d_4s2x40d\", pretrained=True)',\n",
        "    \"resnetv2_50_bit\": 'timm.create_model(\"resnetv2_50x3_bitm\", pretrained=True)',\n",
        "    \"resnetv2_50_21k\": 'timm.create_model(\"resnetv2_50x3_bitm_in21k\", pretrained=True)',\n",
        "    \"resnetv2_101_bit\": 'timm.create_model(\"resnetv2_101x3_bitm\", pretrained=True)',\n",
        "    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n",
        "    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n",
        "    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n",
        "    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n",
        "    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n",
        "    \"vgg11\": \"models.vgg11(pretrained=True)\",\n",
        "    \"vgg19\": \"models.vgg19(pretrained=True)\",\n",
        "    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n",
        "    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n",
        "    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",\n",
        "    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n",
        "    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n",
        "    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n",
        "    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n",
        "    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n",
        "    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n",
        "    \"vit_small\": 'timm.create_model(\"vit_small_patch16_224\", pretrained=True)',\n",
        "    \"vit_base\": 'timm.create_model(\"vit_base_patch16_224\", pretrained=True)',\n",
        "    \"vit_large\": 'timm.create_model(\"vit_large_patch16_224\", pretrained=True)',\n",
        "    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',\n",
        "    \"vit_deit_base\": 'timm.create_model(\"deit_base_patch16_224\", pretrained=True)',\n",
        "    \"vit_deit_distilled\": 'timm.create_model(\"deit_base_distilled_patch16_224\", pretrained=True)',\n",
        "    \"vit_swin_base\": 'timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)',\n",
        "    \"vit_swin_large\": 'timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True)',\n",
        "    \"efficientnet_b7\": 'timm.create_model(\"tf_efficientnet_b7\", pretrained=True)',\n",
        "    \"efficientnet_b5\": 'timm.create_model(\"tf_efficientnet_b5\", pretrained=True)',\n",
        "    \"efficientnet_b3\": 'timm.create_model(\"tf_efficientnet_b3\", pretrained=True)',\n",
        "    \"efficientnet_b1\": 'timm.create_model(\"tf_efficientnet_b1\", pretrained=True)',\n",
        "    \"efficientnetv2_m\": 'timm.create_model(\"tf_efficientnetv2_m\", pretrained=True)',\n",
        "    \"efficientnetv2_l\": 'timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)',\n",
        "    \"efficientnet_b3a\": 'timm.create_model(\"efficientnet_b3a\", pretrained=True)',\n",
        "}\n",
        "\n",
        "\n",
        "def load(name):\n",
        "    return eval(_BACKBONES[name])\n"
      ],
      "metadata": {
        "id": "j5xsCqm-m2CR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#common"
      ],
      "metadata": {
        "id": "fA0dJ1TItqmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "from typing import List\n",
        "from typing import Union\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import scipy.ndimage as ndimage\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FaissNN(object):\n",
        "    def __init__(self, on_gpu: bool = False, num_workers: int = 4) -> None:\n",
        "        \"\"\"FAISS Nearest neighbourhood search.\n",
        "\n",
        "        Args:\n",
        "            on_gpu: If set true, nearest neighbour searches are done on GPU.\n",
        "            num_workers: Number of workers to use with FAISS for similarity search.\n",
        "        \"\"\"\n",
        "        faiss.omp_set_num_threads(num_workers)\n",
        "        self.on_gpu = on_gpu\n",
        "        self.search_index = None\n",
        "\n",
        "    def _gpu_cloner_options(self):\n",
        "        return faiss.GpuClonerOptions()\n",
        "\n",
        "    def _index_to_gpu(self, index):\n",
        "        if self.on_gpu:\n",
        "            # For the non-gpu faiss python package, there is no GpuClonerOptions\n",
        "            # so we can not make a default in the function header.\n",
        "            return faiss.index_cpu_to_gpu(\n",
        "                faiss.StandardGpuResources(), 0, index, self._gpu_cloner_options()\n",
        "            )\n",
        "        return index\n",
        "\n",
        "    def _index_to_cpu(self, index):\n",
        "        if self.on_gpu:\n",
        "            return faiss.index_gpu_to_cpu(index)\n",
        "        return index\n",
        "\n",
        "    def _create_index(self, dimension):\n",
        "        if self.on_gpu:\n",
        "            return faiss.GpuIndexFlatL2(\n",
        "                faiss.StandardGpuResources(), dimension, faiss.GpuIndexFlatConfig()\n",
        "            )\n",
        "        return faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    def fit(self, features: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Adds features to the FAISS search index.\n",
        "\n",
        "        Args:\n",
        "            features: Array of size NxD.\n",
        "        \"\"\"\n",
        "        if self.search_index:\n",
        "            self.reset_index()\n",
        "        self.search_index = self._create_index(features.shape[-1])\n",
        "        self._train(self.search_index, features)\n",
        "        self.search_index.add(features)\n",
        "\n",
        "    def _train(self, _index, _features):\n",
        "        pass\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        n_nearest_neighbours,\n",
        "        query_features: np.ndarray,\n",
        "        index_features: np.ndarray = None,\n",
        "    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Returns distances and indices of nearest neighbour search.\n",
        "\n",
        "        Args:\n",
        "            query_features: Features to retrieve.\n",
        "            index_features: [optional] Index features to search in.\n",
        "        \"\"\"\n",
        "        if index_features is None:\n",
        "            return self.search_index.search(query_features, n_nearest_neighbours)\n",
        "\n",
        "        # Build a search index just for this search.\n",
        "        search_index = self._create_index(index_features.shape[-1])\n",
        "        self._train(search_index, index_features)\n",
        "        search_index.add(index_features)\n",
        "        return search_index.search(query_features, n_nearest_neighbours)\n",
        "\n",
        "    def save(self, filename: str) -> None:\n",
        "        faiss.write_index(self._index_to_cpu(self.search_index), filename)\n",
        "\n",
        "    def load(self, filename: str) -> None:\n",
        "        self.search_index = self._index_to_gpu(faiss.read_index(filename))\n",
        "\n",
        "    def reset_index(self):\n",
        "        if self.search_index:\n",
        "            self.search_index.reset()\n",
        "            self.search_index = None\n",
        "\n",
        "\n",
        "class ApproximateFaissNN(FaissNN):\n",
        "    def _train(self, index, features):\n",
        "        index.train(features)\n",
        "\n",
        "    def _gpu_cloner_options(self):\n",
        "        cloner = faiss.GpuClonerOptions()\n",
        "        cloner.useFloat16 = True\n",
        "        return cloner\n",
        "\n",
        "    def _create_index(self, dimension):\n",
        "        index = faiss.IndexIVFPQ(\n",
        "            faiss.IndexFlatL2(dimension),\n",
        "            dimension,\n",
        "            512,  # n_centroids\n",
        "            64,  # sub-quantizers\n",
        "            8,\n",
        "        )  # nbits per code\n",
        "        return self._index_to_gpu(index)\n",
        "\n",
        "\n",
        "class _BaseMerger:\n",
        "    def __init__(self):\n",
        "        \"\"\"Merges feature embedding by name.\"\"\"\n",
        "\n",
        "    def merge(self, features: list):\n",
        "        features = [self._reduce(feature) for feature in features]\n",
        "        return np.concatenate(features, axis=1)\n",
        "\n",
        "\n",
        "class AverageMerger(_BaseMerger):\n",
        "    @staticmethod\n",
        "    def _reduce(features):\n",
        "        # NxCxWxH -> NxC\n",
        "        return features.reshape([features.shape[0], features.shape[1], -1]).mean(\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "\n",
        "class ConcatMerger(_BaseMerger):\n",
        "    @staticmethod\n",
        "    def _reduce(features):\n",
        "        # NxCxWxH -> NxCWH\n",
        "        return features.reshape(len(features), -1)\n",
        "\n",
        "\n",
        "class Preprocessing(torch.nn.Module):\n",
        "    def __init__(self, input_dims, output_dim):\n",
        "        super(Preprocessing, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.preprocessing_modules = torch.nn.ModuleList()\n",
        "        for input_dim in input_dims:\n",
        "            module = MeanMapper(output_dim)\n",
        "            self.preprocessing_modules.append(module)\n",
        "\n",
        "    def forward(self, features):\n",
        "        _features = []\n",
        "        for module, feature in zip(self.preprocessing_modules, features):\n",
        "            _features.append(module(feature))\n",
        "        return torch.stack(_features, dim=1)\n",
        "\n",
        "\n",
        "class MeanMapper(torch.nn.Module):\n",
        "    def __init__(self, preprocessing_dim):\n",
        "        super(MeanMapper, self).__init__()\n",
        "        self.preprocessing_dim = preprocessing_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        features = features.reshape(len(features), 1, -1)\n",
        "        return F.adaptive_avg_pool1d(features, self.preprocessing_dim).squeeze(1)\n",
        "\n",
        "\n",
        "class Aggregator(torch.nn.Module):\n",
        "    def __init__(self, target_dim):\n",
        "        super(Aggregator, self).__init__()\n",
        "        self.target_dim = target_dim\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"Returns reshaped and average pooled features.\"\"\"\n",
        "        # batchsize x number_of_layers x input_dim -> batchsize x target_dim\n",
        "        features = features.reshape(len(features), 1, -1)\n",
        "        features = F.adaptive_avg_pool1d(features, self.target_dim)\n",
        "        return features.reshape(len(features), -1)\n",
        "\n",
        "\n",
        "class RescaleSegmentor:\n",
        "    def __init__(self, device, target_size=224):\n",
        "        self.device = device\n",
        "        self.target_size = target_size\n",
        "        self.smoothing = 4\n",
        "\n",
        "    def convert_to_segmentation(self, patch_scores):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if isinstance(patch_scores, np.ndarray):\n",
        "                patch_scores = torch.from_numpy(patch_scores)\n",
        "            _scores = patch_scores.to(self.device)\n",
        "            _scores = _scores.unsqueeze(1)\n",
        "            _scores = F.interpolate(\n",
        "                _scores, size=self.target_size, mode=\"bilinear\", align_corners=False\n",
        "            )\n",
        "            _scores = _scores.squeeze(1)\n",
        "            patch_scores = _scores.cpu().numpy()\n",
        "\n",
        "        return [\n",
        "            ndimage.gaussian_filter(patch_score, sigma=self.smoothing)\n",
        "            for patch_score in patch_scores\n",
        "        ]\n",
        "\n",
        "\n",
        "class NetworkFeatureAggregator(torch.nn.Module):\n",
        "    \"\"\"Efficient extraction of network features.\"\"\"\n",
        "\n",
        "    def __init__(self, backbone, layers_to_extract_from, device):\n",
        "        super(NetworkFeatureAggregator, self).__init__()\n",
        "        \"\"\"Extraction of network features.\n",
        "\n",
        "        Runs a network only to the last layer of the list of layers where\n",
        "        network features should be extracted from.\n",
        "\n",
        "        Args:\n",
        "            backbone: torchvision.model\n",
        "            layers_to_extract_from: [list of str]\n",
        "        \"\"\"\n",
        "        self.layers_to_extract_from = layers_to_extract_from\n",
        "        self.backbone = backbone\n",
        "        self.device = device\n",
        "        if not hasattr(backbone, \"hook_handles\"):\n",
        "            self.backbone.hook_handles = []\n",
        "        for handle in self.backbone.hook_handles:\n",
        "            handle.remove()\n",
        "        self.outputs = {}\n",
        "\n",
        "        for extract_layer in layers_to_extract_from:\n",
        "            forward_hook = ForwardHook(\n",
        "                self.outputs, extract_layer, layers_to_extract_from[-1]\n",
        "            )\n",
        "            if \".\" in extract_layer:\n",
        "                extract_block, extract_idx = extract_layer.split(\".\")\n",
        "                network_layer = backbone.__dict__[\"_modules\"][extract_block]\n",
        "                if extract_idx.isnumeric():\n",
        "                    extract_idx = int(extract_idx)\n",
        "                    network_layer = network_layer[extract_idx]\n",
        "                else:\n",
        "                    network_layer = network_layer.__dict__[\"_modules\"][extract_idx]\n",
        "            else:\n",
        "                network_layer = backbone.__dict__[\"_modules\"][extract_layer]\n",
        "\n",
        "            if isinstance(network_layer, torch.nn.Sequential):\n",
        "                self.backbone.hook_handles.append(\n",
        "                    network_layer[-1].register_forward_hook(forward_hook)\n",
        "                )\n",
        "            else:\n",
        "                self.backbone.hook_handles.append(\n",
        "                    network_layer.register_forward_hook(forward_hook)\n",
        "                )\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, images):\n",
        "        self.outputs.clear()\n",
        "        with torch.no_grad():\n",
        "            # The backbone will throw an Exception once it reached the last\n",
        "            # layer to compute features from. Computation will stop there.\n",
        "            try:\n",
        "                _ = self.backbone(images)\n",
        "            except LastLayerToExtractReachedException:\n",
        "                pass\n",
        "        return self.outputs\n",
        "\n",
        "    def feature_dimensions(self, input_shape):\n",
        "        \"\"\"Computes the feature dimensions for all layers given input_shape.\"\"\"\n",
        "        _input = torch.ones([1] + list(input_shape)).to(self.device)\n",
        "        _output = self(_input)\n",
        "        return [_output[layer].shape[1] for layer in self.layers_to_extract_from]\n",
        "\n",
        "\n",
        "class ForwardHook:\n",
        "    def __init__(self, hook_dict, layer_name: str, last_layer_to_extract: str):\n",
        "        self.hook_dict = hook_dict\n",
        "        self.layer_name = layer_name\n",
        "        self.raise_exception_to_break = copy.deepcopy(\n",
        "            layer_name == last_layer_to_extract\n",
        "        )\n",
        "\n",
        "    def __call__(self, module, input, output):\n",
        "        self.hook_dict[self.layer_name] = output\n",
        "        if self.raise_exception_to_break:\n",
        "            raise LastLayerToExtractReachedException()\n",
        "        return None\n",
        "\n",
        "\n",
        "class LastLayerToExtractReachedException(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class NearestNeighbourScorer(object):\n",
        "    def __init__(self, n_nearest_neighbours: int, nn_method=FaissNN(False, 4)) -> None:\n",
        "        \"\"\"\n",
        "        Neearest-Neighbourhood Anomaly Scorer class.\n",
        "\n",
        "        Args:\n",
        "            n_nearest_neighbours: [int] Number of nearest neighbours used to\n",
        "                determine anomalous pixels.\n",
        "            nn_method: Nearest neighbour search method.\n",
        "        \"\"\"\n",
        "        self.feature_merger = ConcatMerger()\n",
        "\n",
        "        self.n_nearest_neighbours = n_nearest_neighbours\n",
        "        self.nn_method = nn_method\n",
        "\n",
        "        self.imagelevel_nn = lambda query: self.nn_method.run(\n",
        "            n_nearest_neighbours, query\n",
        "        )\n",
        "        self.pixelwise_nn = lambda query, index: self.nn_method.run(1, query, index)\n",
        "\n",
        "    def fit(self, detection_features: List[np.ndarray]) -> None:\n",
        "        \"\"\"Calls the fit function of the nearest neighbour method.\n",
        "\n",
        "        Args:\n",
        "            detection_features: [list of np.arrays]\n",
        "                [[bs x d_i] for i in n] Contains a list of\n",
        "                np.arrays for all training images corresponding to respective\n",
        "                features VECTORS (or maps, but will be resized) produced by\n",
        "                some backbone network which should be used for image-level\n",
        "                anomaly detection.\n",
        "        \"\"\"\n",
        "        self.detection_features = self.feature_merger.merge(\n",
        "            detection_features,\n",
        "        )\n",
        "        self.nn_method.fit(self.detection_features)\n",
        "\n",
        "    def predict(\n",
        "        self, query_features: List[np.ndarray]\n",
        "    ) -> Union[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Predicts anomaly score.\n",
        "\n",
        "        Searches for nearest neighbours of test images in all\n",
        "        support training images.\n",
        "\n",
        "        Args:\n",
        "             detection_query_features: [dict of np.arrays] List of np.arrays\n",
        "                 corresponding to the test features generated by\n",
        "                 some backbone network.\n",
        "        \"\"\"\n",
        "        query_features = self.feature_merger.merge(\n",
        "            query_features,\n",
        "        )\n",
        "        query_distances, query_nns = self.imagelevel_nn(query_features)\n",
        "        anomaly_scores = np.mean(query_distances, axis=-1)\n",
        "        return anomaly_scores, query_distances, query_nns\n",
        "\n",
        "    @staticmethod\n",
        "    def _detection_file(folder, prepend=\"\"):\n",
        "        return os.path.join(folder, prepend + \"nnscorer_features.pkl\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _index_file(folder, prepend=\"\"):\n",
        "        return os.path.join(folder, prepend + \"nnscorer_search_index.faiss\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _save(filename, features):\n",
        "        if features is None:\n",
        "            return\n",
        "        with open(filename, \"wb\") as save_file:\n",
        "            pickle.dump(features, save_file, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load(filename: str):\n",
        "        with open(filename, \"rb\") as load_file:\n",
        "            return pickle.load(load_file)\n",
        "\n",
        "    def save(\n",
        "        self,\n",
        "        save_folder: str,\n",
        "        save_features_separately: bool = False,\n",
        "        prepend: str = \"\",\n",
        "    ) -> None:\n",
        "        self.nn_method.save(self._index_file(save_folder, prepend))\n",
        "        if save_features_separately:\n",
        "            self._save(\n",
        "                self._detection_file(save_folder, prepend), self.detection_features\n",
        "            )\n",
        "\n",
        "    def save_and_reset(self, save_folder: str) -> None:\n",
        "        self.save(save_folder)\n",
        "        self.nn_method.reset_index()\n",
        "\n",
        "    def load(self, load_folder: str, prepend: str = \"\") -> None:\n",
        "        self.nn_method.load(self._index_file(load_folder, prepend))\n",
        "        if os.path.exists(self._detection_file(load_folder, prepend)):\n",
        "            self.detection_features = self._load(\n",
        "                self._detection_file(load_folder, prepend)\n",
        "            )\n"
      ],
      "metadata": {
        "id": "yWGB_ey7nskN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#metrics"
      ],
      "metadata": {
        "id": "GSx0hurbtjJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Anomaly metrics.\"\"\"\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def compute_imagewise_retrieval_metrics(\n",
        "    anomaly_prediction_weights, anomaly_ground_truth_labels\n",
        "):\n",
        "    \"\"\"\n",
        "    Computes retrieval statistics (AUROC, FPR, TPR).\n",
        "\n",
        "    Args:\n",
        "        anomaly_prediction_weights: [np.array or list] [N] Assignment weights\n",
        "                                    per image. Higher indicates higher\n",
        "                                    probability of being an anomaly.\n",
        "        anomaly_ground_truth_labels: [np.array or list] [N] Binary labels - 1\n",
        "                                    if image is an anomaly, 0 if not.\n",
        "    \"\"\"\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(\n",
        "        anomaly_ground_truth_labels, anomaly_prediction_weights\n",
        "    )\n",
        "    auroc = metrics.roc_auc_score(\n",
        "        anomaly_ground_truth_labels, anomaly_prediction_weights\n",
        "    )\n",
        "    return {\"auroc\": auroc, \"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds}\n",
        "\n",
        "\n",
        "def compute_pixelwise_retrieval_metrics(anomaly_segmentations, ground_truth_masks):\n",
        "    \"\"\"\n",
        "    Computes pixel-wise statistics (AUROC, FPR, TPR) for anomaly segmentations\n",
        "    and ground truth segmentation masks.\n",
        "\n",
        "    Args:\n",
        "        anomaly_segmentations: [list of np.arrays or np.array] [NxHxW] Contains\n",
        "                                generated segmentation masks.\n",
        "        ground_truth_masks: [list of np.arrays or np.array] [NxHxW] Contains\n",
        "                            predefined ground truth segmentation masks\n",
        "    \"\"\"\n",
        "    if isinstance(anomaly_segmentations, list):\n",
        "        anomaly_segmentations = np.stack(anomaly_segmentations)\n",
        "    if isinstance(ground_truth_masks, list):\n",
        "        ground_truth_masks = np.stack(ground_truth_masks)\n",
        "\n",
        "    flat_anomaly_segmentations = anomaly_segmentations.ravel()\n",
        "    flat_ground_truth_masks = ground_truth_masks.ravel()\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(\n",
        "        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n",
        "    )\n",
        "    auroc = metrics.roc_auc_score(\n",
        "        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n",
        "    )\n",
        "\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
        "        flat_ground_truth_masks.astype(int), flat_anomaly_segmentations\n",
        "    )\n",
        "    F1_scores = np.divide(\n",
        "        2 * precision * recall,\n",
        "        precision + recall,\n",
        "        out=np.zeros_like(precision),\n",
        "        where=(precision + recall) != 0,\n",
        "    )\n",
        "\n",
        "    optimal_threshold = thresholds[np.argmax(F1_scores)]\n",
        "    predictions = (flat_anomaly_segmentations >= optimal_threshold).astype(int)\n",
        "    fpr_optim = np.mean(predictions > flat_ground_truth_masks)\n",
        "    fnr_optim = np.mean(predictions < flat_ground_truth_masks)\n",
        "\n",
        "    return {\n",
        "        \"auroc\": auroc,\n",
        "        \"fpr\": fpr,\n",
        "        \"tpr\": tpr,\n",
        "        \"optimal_threshold\": optimal_threshold,\n",
        "        \"optimal_fpr\": fpr_optim,\n",
        "        \"optimal_fnr\": fnr_optim,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "eUiMexGVnsmb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sampler"
      ],
      "metadata": {
        "id": "JEJreVA4te-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "from typing import Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "\n",
        "class IdentitySampler:\n",
        "    def run(\n",
        "        self, features: Union[torch.Tensor, np.ndarray]\n",
        "    ) -> Union[torch.Tensor, np.ndarray]:\n",
        "        return features\n",
        "\n",
        "\n",
        "class BaseSampler(abc.ABC):\n",
        "    def __init__(self, percentage: float):\n",
        "        if not 0 < percentage < 1:\n",
        "            raise ValueError(\"Percentage value not in (0, 1).\")\n",
        "        self.percentage = percentage\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def run(\n",
        "        self, features: Union[torch.Tensor, np.ndarray]\n",
        "    ) -> Union[torch.Tensor, np.ndarray]:\n",
        "        pass\n",
        "\n",
        "    def _store_type(self, features: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "        self.features_is_numpy = isinstance(features, np.ndarray)\n",
        "        if not self.features_is_numpy:\n",
        "            self.features_device = features.device\n",
        "\n",
        "    def _restore_type(self, features: torch.Tensor) -> Union[torch.Tensor, np.ndarray]:\n",
        "        if self.features_is_numpy:\n",
        "            return features.cpu().numpy()\n",
        "        return features.to(self.features_device)\n",
        "\n",
        "\n",
        "class GreedyCoresetSampler(BaseSampler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        percentage: float,\n",
        "        device: torch.device,\n",
        "        dimension_to_project_features_to=128,\n",
        "    ):\n",
        "        \"\"\"Greedy Coreset sampling base class.\"\"\"\n",
        "        super().__init__(percentage)\n",
        "\n",
        "        self.device = device\n",
        "        self.dimension_to_project_features_to = dimension_to_project_features_to\n",
        "\n",
        "    def _reduce_features(self, features):\n",
        "        if features.shape[1] == self.dimension_to_project_features_to:\n",
        "            return features\n",
        "        mapper = torch.nn.Linear(\n",
        "            features.shape[1], self.dimension_to_project_features_to, bias=False\n",
        "        )\n",
        "        _ = mapper.to(self.device)\n",
        "        features = features.to(self.device)\n",
        "        return mapper(features)\n",
        "\n",
        "    def run(\n",
        "        self, features: Union[torch.Tensor, np.ndarray]\n",
        "    ) -> Union[torch.Tensor, np.ndarray]:\n",
        "        \"\"\"Subsamples features using Greedy Coreset.\n",
        "\n",
        "        Args:\n",
        "            features: [N x D]\n",
        "        \"\"\"\n",
        "        if self.percentage == 1:\n",
        "            return features\n",
        "        self._store_type(features)\n",
        "        if isinstance(features, np.ndarray):\n",
        "            features = torch.from_numpy(features)\n",
        "        reduced_features = self._reduce_features(features)\n",
        "        sample_indices = self._compute_greedy_coreset_indices(reduced_features)\n",
        "        features = features[sample_indices]\n",
        "        return self._restore_type(features)\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_batchwise_differences(\n",
        "        matrix_a: torch.Tensor, matrix_b: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Computes batchwise Euclidean distances using PyTorch.\"\"\"\n",
        "        a_times_a = matrix_a.unsqueeze(1).bmm(matrix_a.unsqueeze(2)).reshape(-1, 1)\n",
        "        b_times_b = matrix_b.unsqueeze(1).bmm(matrix_b.unsqueeze(2)).reshape(1, -1)\n",
        "        a_times_b = matrix_a.mm(matrix_b.T)\n",
        "\n",
        "        return (-2 * a_times_b + a_times_a + b_times_b).clamp(0, None).sqrt()\n",
        "\n",
        "    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"Runs iterative greedy coreset selection.\n",
        "\n",
        "        Args:\n",
        "            features: [NxD] input feature bank to sample.\n",
        "        \"\"\"\n",
        "        distance_matrix = self._compute_batchwise_differences(features, features)\n",
        "        coreset_anchor_distances = torch.norm(distance_matrix, dim=1)\n",
        "\n",
        "        coreset_indices = []\n",
        "        num_coreset_samples = int(len(features) * self.percentage)\n",
        "\n",
        "        for _ in range(num_coreset_samples):\n",
        "            select_idx = torch.argmax(coreset_anchor_distances).item()\n",
        "            coreset_indices.append(select_idx)\n",
        "\n",
        "            coreset_select_distance = distance_matrix[\n",
        "                :, select_idx : select_idx + 1  # noqa E203\n",
        "            ]\n",
        "            coreset_anchor_distances = torch.cat(\n",
        "                [coreset_anchor_distances.unsqueeze(-1), coreset_select_distance], dim=1\n",
        "            )\n",
        "            coreset_anchor_distances = torch.min(coreset_anchor_distances, dim=1).values\n",
        "\n",
        "        return np.array(coreset_indices)\n",
        "\n",
        "\n",
        "class ApproximateGreedyCoresetSampler(GreedyCoresetSampler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        percentage: float,\n",
        "        device: torch.device,\n",
        "        number_of_starting_points: int = 10,\n",
        "        dimension_to_project_features_to: int = 128,\n",
        "    ):\n",
        "        \"\"\"Approximate Greedy Coreset sampling base class.\"\"\"\n",
        "        self.number_of_starting_points = number_of_starting_points\n",
        "        super().__init__(percentage, device, dimension_to_project_features_to)\n",
        "\n",
        "    def _compute_greedy_coreset_indices(self, features: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"Runs approximate iterative greedy coreset selection.\n",
        "\n",
        "        This greedy coreset implementation does not require computation of the\n",
        "        full N x N distance matrix and thus requires a lot less memory, however\n",
        "        at the cost of increased sampling times.\n",
        "\n",
        "        Args:\n",
        "            features: [NxD] input feature bank to sample.\n",
        "        \"\"\"\n",
        "        number_of_starting_points = np.clip(\n",
        "            self.number_of_starting_points, None, len(features)\n",
        "        )\n",
        "        start_points = np.random.choice(\n",
        "            len(features), number_of_starting_points, replace=False\n",
        "        ).tolist()\n",
        "\n",
        "        approximate_distance_matrix = self._compute_batchwise_differences(\n",
        "            features, features[start_points]\n",
        "        )\n",
        "        approximate_coreset_anchor_distances = torch.mean(\n",
        "            approximate_distance_matrix, axis=-1\n",
        "        ).reshape(-1, 1)\n",
        "        coreset_indices = []\n",
        "        num_coreset_samples = int(len(features) * self.percentage)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in tqdm.tqdm(range(num_coreset_samples), desc=\"Subsampling...\"):\n",
        "                select_idx = torch.argmax(approximate_coreset_anchor_distances).item()\n",
        "                coreset_indices.append(select_idx)\n",
        "                coreset_select_distance = self._compute_batchwise_differences(\n",
        "                    features, features[select_idx : select_idx + 1]  # noqa: E203\n",
        "                )\n",
        "                approximate_coreset_anchor_distances = torch.cat(\n",
        "                    [approximate_coreset_anchor_distances, coreset_select_distance],\n",
        "                    dim=-1,\n",
        "                )\n",
        "                approximate_coreset_anchor_distances = torch.min(\n",
        "                    approximate_coreset_anchor_distances, dim=1\n",
        "                ).values.reshape(-1, 1)\n",
        "\n",
        "        return np.array(coreset_indices)\n",
        "\n",
        "\n",
        "class RandomSampler(BaseSampler):\n",
        "    def __init__(self, percentage: float):\n",
        "        super().__init__(percentage)\n",
        "\n",
        "    def run(\n",
        "        self, features: Union[torch.Tensor, np.ndarray]\n",
        "    ) -> Union[torch.Tensor, np.ndarray]:\n",
        "        \"\"\"Randomly samples input feature collection.\n",
        "\n",
        "        Args:\n",
        "            features: [N x D]\n",
        "        \"\"\"\n",
        "        num_random_samples = int(len(features) * self.percentage)\n",
        "        subset_indices = np.random.choice(\n",
        "            len(features), num_random_samples, replace=False\n",
        "        )\n",
        "        subset_indices = np.array(subset_indices)\n",
        "        return features[subset_indices]\n"
      ],
      "metadata": {
        "id": "pS1V0ct2q_jT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#patchcore"
      ],
      "metadata": {
        "id": "rxcVTKf_tcP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PatchCore(torch.nn.Module):\n",
        "    def __init__(self, device):\n",
        "        \"\"\"PatchCore anomaly detection class.\"\"\"\n",
        "        super(PatchCore, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        backbone,\n",
        "        layers_to_extract_from,\n",
        "        device,\n",
        "        input_shape,\n",
        "        pretrain_embed_dimension,\n",
        "        target_embed_dimension,\n",
        "        patchsize=3,\n",
        "        patchstride=1,\n",
        "        anomaly_score_num_nn=1,\n",
        "        featuresampler=IdentitySampler(),\n",
        "        nn_method=FaissNN(False, 4),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.backbone = backbone.to(device)\n",
        "        self.layers_to_extract_from = layers_to_extract_from\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        self.device = device\n",
        "        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n",
        "\n",
        "        self.forward_modules = torch.nn.ModuleDict({})\n",
        "\n",
        "        feature_aggregator = NetworkFeatureAggregator(\n",
        "            self.backbone, self.layers_to_extract_from, self.device\n",
        "        )\n",
        "        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n",
        "        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n",
        "\n",
        "        preprocessing = Preprocessing(\n",
        "            feature_dimensions, pretrain_embed_dimension\n",
        "        )\n",
        "        self.forward_modules[\"preprocessing\"] = preprocessing\n",
        "\n",
        "        self.target_embed_dimension = target_embed_dimension\n",
        "        preadapt_aggregator = Aggregator(\n",
        "            target_dim=target_embed_dimension\n",
        "        )\n",
        "\n",
        "        _ = preadapt_aggregator.to(self.device)\n",
        "\n",
        "        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n",
        "\n",
        "        self.anomaly_scorer = NearestNeighbourScorer(\n",
        "            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n",
        "        )\n",
        "\n",
        "        self.anomaly_segmentor = RescaleSegmentor(\n",
        "            device=self.device, target_size=input_shape[-2:]\n",
        "        )\n",
        "\n",
        "        self.featuresampler = featuresampler\n",
        "\n",
        "    def embed(self, data):\n",
        "        if isinstance(data, torch.utils.data.DataLoader):\n",
        "            features = []\n",
        "            for image in data:\n",
        "                if isinstance(image, dict):\n",
        "                    image = image[\"image\"]\n",
        "                with torch.no_grad():\n",
        "                    input_image = image.to(torch.float).to(self.device)\n",
        "                    features.append(self._embed(input_image))\n",
        "            return features\n",
        "        return self._embed(data)\n",
        "\n",
        "    def _embed(self, images, detach=True, provide_patch_shapes=False):\n",
        "        \"\"\"Returns feature embeddings for images.\"\"\"\n",
        "\n",
        "        def _detach(features):\n",
        "            if detach:\n",
        "                return [x.detach().cpu().numpy() for x in features]\n",
        "            return features\n",
        "\n",
        "        _ = self.forward_modules[\"feature_aggregator\"].eval()\n",
        "        with torch.no_grad():\n",
        "            features = self.forward_modules[\"feature_aggregator\"](images)\n",
        "\n",
        "        features = [features[layer] for layer in self.layers_to_extract_from]\n",
        "\n",
        "        features = [\n",
        "            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n",
        "        ]\n",
        "        patch_shapes = [x[1] for x in features]\n",
        "        features = [x[0] for x in features]\n",
        "        ref_num_patches = patch_shapes[0]\n",
        "\n",
        "        for i in range(1, len(features)):\n",
        "            _features = features[i]\n",
        "            patch_dims = patch_shapes[i]\n",
        "\n",
        "            # TODO(pgehler): Add comments\n",
        "            _features = _features.reshape(\n",
        "                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n",
        "            )\n",
        "            _features = _features.permute(0, -3, -2, -1, 1, 2)\n",
        "            perm_base_shape = _features.shape\n",
        "            _features = _features.reshape(-1, *_features.shape[-2:])\n",
        "            _features = F.interpolate(\n",
        "                _features.unsqueeze(1),\n",
        "                size=(ref_num_patches[0], ref_num_patches[1]),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            _features = _features.squeeze(1)\n",
        "            _features = _features.reshape(\n",
        "                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n",
        "            )\n",
        "            _features = _features.permute(0, -2, -1, 1, 2, 3)\n",
        "            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n",
        "            features[i] = _features\n",
        "        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n",
        "\n",
        "        # As different feature backbones & patching provide differently\n",
        "        # sized features, these are brought into the correct form here.\n",
        "        features = self.forward_modules[\"preprocessing\"](features)\n",
        "        features = self.forward_modules[\"preadapt_aggregator\"](features)\n",
        "\n",
        "        if provide_patch_shapes:\n",
        "            return _detach(features), patch_shapes\n",
        "        return _detach(features)\n",
        "\n",
        "    def fit(self, training_data):\n",
        "        \"\"\"PatchCore training.\n",
        "\n",
        "        This function computes the embeddings of the training data and fills the\n",
        "        memory bank of SPADE.\n",
        "        \"\"\"\n",
        "        self._fill_memory_bank(training_data)\n",
        "\n",
        "    def _fill_memory_bank(self, input_data):\n",
        "        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        def _image_to_features(input_image):\n",
        "            with torch.no_grad():\n",
        "                input_image = input_image.to(torch.float).to(self.device)\n",
        "                return self._embed(input_image)\n",
        "\n",
        "        features = []\n",
        "        with tqdm.tqdm(\n",
        "            input_data, desc=\"Computing support features...\", position=1, leave=False\n",
        "        ) as data_iterator:\n",
        "            for image in data_iterator:\n",
        "                if isinstance(image, dict):\n",
        "                    image = image[\"image\"]\n",
        "                features.append(_image_to_features(image))\n",
        "\n",
        "        features = np.concatenate(features, axis=0)\n",
        "        features = self.featuresampler.run(features)\n",
        "\n",
        "        self.anomaly_scorer.fit(detection_features=[features])\n",
        "\n",
        "    def predict(self, data):\n",
        "        if isinstance(data, torch.utils.data.DataLoader):\n",
        "            return self._predict_dataloader(data)\n",
        "        return self._predict(data)\n",
        "\n",
        "    def _predict_dataloader(self, dataloader):\n",
        "        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        scores = []\n",
        "        masks = []\n",
        "        labels_gt = []\n",
        "        masks_gt = []\n",
        "        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n",
        "            for image in data_iterator:\n",
        "                if isinstance(image, dict):\n",
        "                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n",
        "                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n",
        "                    image = image[\"image\"]\n",
        "                _scores, _masks = self._predict(image)\n",
        "                for score, mask in zip(_scores, _masks):\n",
        "                    scores.append(score)\n",
        "                    masks.append(mask)\n",
        "        return scores, masks, labels_gt, masks_gt\n",
        "\n",
        "    def _predict(self, images):\n",
        "        \"\"\"Infer score and mask for a batch of images.\"\"\"\n",
        "        images = images.to(torch.float).to(self.device)\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        batchsize = images.shape[0]\n",
        "        with torch.no_grad():\n",
        "            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n",
        "            features = np.asarray(features)\n",
        "\n",
        "            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n",
        "            image_scores = self.patch_maker.unpatch_scores(\n",
        "                image_scores, batchsize=batchsize\n",
        "            )\n",
        "            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n",
        "            image_scores = self.patch_maker.score(image_scores)\n",
        "\n",
        "            patch_scores = self.patch_maker.unpatch_scores(\n",
        "                patch_scores, batchsize=batchsize\n",
        "            )\n",
        "            scales = patch_shapes[0]\n",
        "            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n",
        "\n",
        "            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n",
        "\n",
        "        return [score for score in image_scores], [mask for mask in masks]\n",
        "\n",
        "    @staticmethod\n",
        "    def _params_file(filepath, prepend=\"\"):\n",
        "        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n",
        "\n",
        "    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n",
        "        LOGGER.info(\"Saving PatchCore data.\")\n",
        "        self.anomaly_scorer.save(\n",
        "            save_path, save_features_separately=False, prepend=prepend\n",
        "        )\n",
        "        patchcore_params = {\n",
        "            \"backbone.name\": self.backbone.name,\n",
        "            \"layers_to_extract_from\": self.layers_to_extract_from,\n",
        "            \"input_shape\": self.input_shape,\n",
        "            \"pretrain_embed_dimension\": self.forward_modules[\n",
        "                \"preprocessing\"\n",
        "            ].output_dim,\n",
        "            \"target_embed_dimension\": self.forward_modules[\n",
        "                \"preadapt_aggregator\"\n",
        "            ].target_dim,\n",
        "            \"patchsize\": self.patch_maker.patchsize,\n",
        "            \"patchstride\": self.patch_maker.stride,\n",
        "            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n",
        "        }\n",
        "        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n",
        "            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load_from_path(\n",
        "        self,\n",
        "        load_path: str,\n",
        "        device: torch.device,\n",
        "        nn_method: FaissNN(False, 4),\n",
        "        prepend: str = \"\",\n",
        "    ) -> None:\n",
        "        LOGGER.info(\"Loading and initializing PatchCore.\")\n",
        "        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n",
        "            patchcore_params = pickle.load(load_file)\n",
        "        patchcore_params[\"backbone\"] = load(\n",
        "            patchcore_params[\"backbone.name\"]\n",
        "        )\n",
        "        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n",
        "        del patchcore_params[\"backbone.name\"]\n",
        "        self.load(**patchcore_params, device=device, nn_method=nn_method)\n",
        "\n",
        "        self.anomaly_scorer.load(load_path, prepend)\n",
        "\n",
        "\n",
        "# Image handling classes.\n",
        "class PatchMaker:\n",
        "    def __init__(self, patchsize, stride=None):\n",
        "        self.patchsize = patchsize\n",
        "        self.stride = stride\n",
        "\n",
        "    def patchify(self, features, return_spatial_info=False):\n",
        "        \"\"\"Convert a tensor into a tensor of respective patches.\n",
        "        Args:\n",
        "            x: [torch.Tensor, bs x c x w x h]\n",
        "        Returns:\n",
        "            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n",
        "            patchsize]\n",
        "        \"\"\"\n",
        "        padding = int((self.patchsize - 1) / 2)\n",
        "        unfolder = torch.nn.Unfold(\n",
        "            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n",
        "        )\n",
        "        unfolded_features = unfolder(features)\n",
        "        number_of_total_patches = []\n",
        "        for s in features.shape[-2:]:\n",
        "            n_patches = (\n",
        "                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n",
        "            ) / self.stride + 1\n",
        "            number_of_total_patches.append(int(n_patches))\n",
        "        unfolded_features = unfolded_features.reshape(\n",
        "            *features.shape[:2], self.patchsize, self.patchsize, -1\n",
        "        )\n",
        "        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n",
        "\n",
        "        if return_spatial_info:\n",
        "            return unfolded_features, number_of_total_patches\n",
        "        return unfolded_features\n",
        "\n",
        "    def unpatch_scores(self, x, batchsize):\n",
        "        return x.reshape(batchsize, -1, *x.shape[1:])\n",
        "\n",
        "    def score(self, x):\n",
        "        was_numpy = False\n",
        "        if isinstance(x, np.ndarray):\n",
        "            was_numpy = True\n",
        "            x = torch.from_numpy(x)\n",
        "        while x.ndim > 1:\n",
        "            x = torch.max(x, dim=-1).values\n",
        "        if was_numpy:\n",
        "            return x.numpy()\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Dvu_B94zqXTA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils"
      ],
      "metadata": {
        "id": "u0VawNbptWp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def plot_segmentation_images(\n",
        "    savefolder,\n",
        "    image_paths,\n",
        "    segmentations,\n",
        "    anomaly_scores=None,\n",
        "    mask_paths=None,\n",
        "    image_transform=lambda x: x,\n",
        "    mask_transform=lambda x: x,\n",
        "    save_depth=4,\n",
        "):\n",
        "    \"\"\"Generate anomaly segmentation images.\n",
        "\n",
        "    Args:\n",
        "        image_paths: List[str] List of paths to images.\n",
        "        segmentations: [List[np.ndarray]] Generated anomaly segmentations.\n",
        "        anomaly_scores: [List[float]] Anomaly scores for each image.\n",
        "        mask_paths: [List[str]] List of paths to ground truth masks.\n",
        "        image_transform: [function or lambda] Optional transformation of images.\n",
        "        mask_transform: [function or lambda] Optional transformation of masks.\n",
        "        save_depth: [int] Number of path-strings to use for image savenames.\n",
        "    \"\"\"\n",
        "    if mask_paths is None:\n",
        "        mask_paths = [\"-1\" for _ in range(len(image_paths))]\n",
        "    masks_provided = mask_paths[0] != \"-1\"\n",
        "    if anomaly_scores is None:\n",
        "        anomaly_scores = [\"-1\" for _ in range(len(image_paths))]\n",
        "\n",
        "    os.makedirs(savefolder, exist_ok=True)\n",
        "\n",
        "    for image_path, mask_path, anomaly_score, segmentation in tqdm.tqdm(\n",
        "        zip(image_paths, mask_paths, anomaly_scores, segmentations),\n",
        "        total=len(image_paths),\n",
        "        desc=\"Generating Segmentation Images...\",\n",
        "        leave=False,\n",
        "    ):\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "        image = image_transform(image)\n",
        "        if not isinstance(image, np.ndarray):\n",
        "            image = image.numpy()\n",
        "\n",
        "        if masks_provided:\n",
        "            if mask_path is not None:\n",
        "                mask = PIL.Image.open(mask_path).convert(\"RGB\")\n",
        "                mask = mask_transform(mask)\n",
        "                if not isinstance(mask, np.ndarray):\n",
        "                    mask = mask.numpy()\n",
        "            else:\n",
        "                mask = np.zeros_like(image)\n",
        "\n",
        "        savename = image_path.split(\"/\")\n",
        "        savename = \"_\".join(savename[-save_depth:])\n",
        "        savename = os.path.join(savefolder, savename)\n",
        "        f, axes = plt.subplots(1, 2 + int(masks_provided))\n",
        "        axes[0].imshow(image.transpose(1, 2, 0))\n",
        "        axes[1].imshow(mask.transpose(1, 2, 0))\n",
        "        axes[2].imshow(segmentation)\n",
        "        f.set_size_inches(3 * (2 + int(masks_provided)), 3)\n",
        "        f.tight_layout()\n",
        "        f.savefig(savename)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def create_storage_folder(\n",
        "    main_folder_path, project_folder, group_folder, mode=\"iterate\"\n",
        "):\n",
        "    os.makedirs(main_folder_path, exist_ok=True)\n",
        "    project_path = os.path.join(main_folder_path, project_folder)\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "    save_path = os.path.join(project_path, group_folder)\n",
        "    if mode == \"iterate\":\n",
        "        counter = 0\n",
        "        while os.path.exists(save_path):\n",
        "            save_path = os.path.join(project_path, group_folder + \"_\" + str(counter))\n",
        "            counter += 1\n",
        "        os.makedirs(save_path)\n",
        "    elif mode == \"overwrite\":\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    return save_path\n",
        "\n",
        "\n",
        "def set_torch_device(gpu_ids):\n",
        "    \"\"\"Returns correct torch.device.\n",
        "\n",
        "    Args:\n",
        "        gpu_ids: [list] list of gpu ids. If empty, cpu is used.\n",
        "    \"\"\"\n",
        "    if len(gpu_ids):\n",
        "        # os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "        # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_ids[0])\n",
        "        return torch.device(\"cuda:{}\".format(gpu_ids[0]))\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def fix_seeds(seed, with_torch=True, with_cuda=True):\n",
        "    \"\"\"Fixed available seeds for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed: [int] Seed value.\n",
        "        with_torch: Flag. If true, torch-related seeds are fixed.\n",
        "        with_cuda: Flag. If true, torch+cuda-related seeds are fixed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if with_torch:\n",
        "        torch.manual_seed(seed)\n",
        "    if with_cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def compute_and_store_final_results(\n",
        "    results_path,\n",
        "    results,\n",
        "    row_names=None,\n",
        "    column_names=[\n",
        "        \"Instance AUROC\",\n",
        "        \"Full Pixel AUROC\",\n",
        "        \"Full PRO\",\n",
        "        \"Anomaly Pixel AUROC\",\n",
        "        \"Anomaly PRO\",\n",
        "    ],\n",
        "):\n",
        "    \"\"\"Store computed results as CSV file.\n",
        "\n",
        "    Args:\n",
        "        results_path: [str] Where to store result csv.\n",
        "        results: [List[List]] List of lists containing results per dataset,\n",
        "                 with results[i][0] == 'dataset_name' and results[i][1:6] =\n",
        "                 [instance_auroc, full_pixelwisew_auroc, full_pro,\n",
        "                 anomaly-only_pw_auroc, anomaly-only_pro]\n",
        "    \"\"\"\n",
        "    if row_names is not None:\n",
        "        assert len(row_names) == len(results), \"#Rownames != #Result-rows.\"\n",
        "\n",
        "    mean_metrics = {}\n",
        "    for i, result_key in enumerate(column_names):\n",
        "        mean_metrics[result_key] = np.mean([x[i] for x in results])\n",
        "        LOGGER.info(\"{0}: {1:3.3f}\".format(result_key, mean_metrics[result_key]))\n",
        "\n",
        "    savename = os.path.join(results_path, \"results.csv\")\n",
        "    with open(savename, \"w\") as csv_file:\n",
        "        csv_writer = csv.writer(csv_file, delimiter=\",\")\n",
        "        header = column_names\n",
        "        if row_names is not None:\n",
        "            header = [\"Row Names\"] + header\n",
        "\n",
        "        csv_writer.writerow(header)\n",
        "        for i, result_list in enumerate(results):\n",
        "            csv_row = result_list\n",
        "            if row_names is not None:\n",
        "                csv_row = [row_names[i]] + result_list\n",
        "            csv_writer.writerow(csv_row)\n",
        "        mean_scores = list(mean_metrics.values())\n",
        "        if row_names is not None:\n",
        "            mean_scores = [\"Mean\"] + mean_scores\n",
        "        csv_writer.writerow(mean_scores)\n",
        "\n",
        "    mean_metrics = {\"mean_{0}\".format(key): item for key, item in mean_metrics.items()}\n",
        "    return mean_metrics\n"
      ],
      "metadata": {
        "id": "5CkRQC3-qXaS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test partially patchcore"
      ],
      "metadata": {
        "id": "gcNDFx-ru0uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def _dummy_features(number_of_examples, shape_of_examples):\n",
        "    return torch.Tensor(\n",
        "        np.stack(number_of_examples * [np.ones(shape_of_examples)], axis=0)\n",
        "    )\n",
        "\n",
        "\n",
        "def _dummy_constant_dataloader(number_of_examples, shape_of_examples):\n",
        "    features = _dummy_features(number_of_examples, shape_of_examples)\n",
        "    return torch.utils.data.DataLoader(features, batch_size=1)\n",
        "\n",
        "\n",
        "def _dummy_various_features(number_of_examples, shape_of_examples):\n",
        "    images = torch.ones((number_of_examples, *shape_of_examples))\n",
        "    multiplier = torch.arange(number_of_examples) / float(number_of_examples)\n",
        "    for _ in range(images.ndim - 1):\n",
        "        multiplier = multiplier.unsqueeze(-1)\n",
        "    return multiplier * images\n",
        "\n",
        "\n",
        "def _dummy_various_dataloader(number_of_examples, shape_of_examples):\n",
        "    features = _dummy_various_features(number_of_examples, shape_of_examples)\n",
        "    return torch.utils.data.DataLoader(features, batch_size=1)\n",
        "\n",
        "\n",
        "def _dummy_images(number_of_examples, image_shape):\n",
        "    torch.manual_seed(0)\n",
        "    return torch.rand([number_of_examples, *image_shape])\n",
        "\n",
        "\n",
        "def _dummy_image_random_dataloader(number_of_examples, image_shape):\n",
        "    images = _dummy_images(number_of_examples, image_shape)\n",
        "    return torch.utils.data.DataLoader(images, batch_size=4)\n",
        "\n",
        "\n",
        "def _standard_patchcore(image_dimension):\n",
        "    patchcore_instance = PatchCore(torch.device(\"cpu\"))\n",
        "    backbone = models.wide_resnet50_2(pretrained=False)\n",
        "    backbone.name, backbone.seed = \"wideresnet50\", 0\n",
        "    patchcore_instance.load(\n",
        "        backbone=backbone,\n",
        "        layers_to_extract_from=[\"layer2\", \"layer3\"],\n",
        "        device=torch.device(\"cpu\"),\n",
        "        input_shape=[3, image_dimension, image_dimension],\n",
        "        pretrain_embed_dimension=1024,\n",
        "        target_embed_dimension=1024,\n",
        "        patchsize=3,\n",
        "        patchstride=1,\n",
        "        spade_nn=2,\n",
        "    )\n",
        "    return patchcore_instance\n",
        "\n",
        "\n",
        "def _load_patchcore_from_path(load_path):\n",
        "    patchcore_instance = PatchCore(torch.device(\"cpu\"))\n",
        "    patchcore_instance.load_from_path(\n",
        "        load_path=load_path,\n",
        "        device=torch.device(\"cpu\"),\n",
        "        prepend=\"temp_patchcore\",\n",
        "        nn_method=FaissNN(False, 4),\n",
        "    )\n",
        "    return patchcore_instance\n",
        "\n",
        "\n",
        "def _approximate_greedycoreset_sampler_with_reduction(\n",
        "    sampling_percentage, johnsonlindenstrauss_dim\n",
        "):\n",
        "    return ApproximateGreedyCoresetSampler(\n",
        "        percentage=sampling_percentage,\n",
        "        device=torch.device(\"cpu\"),\n",
        "        number_of_starting_points=10,\n",
        "        dimension_to_project_features_to=johnsonlindenstrauss_dim,\n",
        "    )\n",
        "\n",
        "\n",
        "def test_dummy_patchcore():\n",
        "    image_dimension = 112\n",
        "    model = _standard_patchcore(image_dimension)\n",
        "    training_dataloader = _dummy_constant_dataloader(\n",
        "        4, [3, image_dimension, image_dimension]\n",
        "    )\n",
        "    print(model.featuresampler)\n",
        "    model.fit(training_dataloader)\n",
        "\n",
        "    test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n",
        "    scores, masks = model.predict(test_features)\n",
        "\n",
        "    assert all([score > 0 for score in scores])\n",
        "    for mask in masks:\n",
        "        assert np.all(mask.shape == (image_dimension, image_dimension))\n",
        "\n",
        "\n",
        "def test_patchcore_on_dataloader():\n",
        "    \"\"\"Test PatchCore on dataloader and assure training scores are zero.\"\"\"\n",
        "    image_dimension = 112\n",
        "    model = _standard_patchcore(image_dimension)\n",
        "\n",
        "    training_dataloader = _dummy_constant_dataloader(\n",
        "        4, [3, image_dimension, image_dimension]\n",
        "    )\n",
        "    model.fit(training_dataloader)\n",
        "    scores, masks, labels_gt, masks_gt = model.predict(training_dataloader)\n",
        "\n",
        "    assert all([score < 1e-3 for score in scores])\n",
        "    for mask, mask_gt in zip(masks, masks_gt):\n",
        "        assert np.all(mask.shape == (image_dimension, image_dimension))\n",
        "        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n",
        "\n",
        "\n",
        "def test_patchcore_load_and_saveing(tmpdir):\n",
        "    image_dimension = 112\n",
        "    model = _standard_patchcore(image_dimension)\n",
        "\n",
        "    training_dataloader = _dummy_constant_dataloader(\n",
        "        4, [3, image_dimension, image_dimension]\n",
        "    )\n",
        "    model.fit(training_dataloader)\n",
        "    model.save_to_path(tmpdir, \"temp_patchcore\")\n",
        "\n",
        "    test_features = torch.Tensor(\n",
        "        1.234 * np.ones([2, 3, image_dimension, image_dimension])\n",
        "    )\n",
        "    scores, masks = model.predict(test_features)\n",
        "    other_scores, other_masks = model.predict(test_features)\n",
        "\n",
        "    assert np.all(scores == other_scores)\n",
        "    for mask, other_mask in zip(masks, other_masks):\n",
        "        assert np.all(mask == other_mask)\n",
        "\n",
        "\n",
        "def test_patchcore_real_data():\n",
        "    image_dimension = 112\n",
        "    sampling_percentage = 0.1\n",
        "    model = _standard_patchcore(image_dimension)\n",
        "    model.sampler = _approximate_greedycoreset_sampler_with_reduction(\n",
        "        sampling_percentage=sampling_percentage,\n",
        "        johnsonlindenstrauss_dim=64,\n",
        "    )\n",
        "\n",
        "    num_dummy_train_images = 50\n",
        "    training_dataloader = _dummy_various_dataloader(\n",
        "        num_dummy_train_images, [3, image_dimension, image_dimension]\n",
        "    )\n",
        "    model.fit(training_dataloader)\n",
        "\n",
        "    num_dummy_test_images = 5\n",
        "    test_dataloader = _dummy_various_dataloader(\n",
        "        num_dummy_test_images, [3, image_dimension, image_dimension]\n",
        "    )\n",
        "    scores, masks, labels_gt, masks_gt = model.predict(test_dataloader)\n",
        "\n",
        "    for mask, mask_gt in zip(masks, masks_gt):\n",
        "        assert np.all(mask.shape == (image_dimension, image_dimension))\n",
        "        assert np.all(mask_gt.shape == (image_dimension, image_dimension))\n",
        "\n",
        "    assert len(scores) == 5\n"
      ],
      "metadata": {
        "id": "QowxnIbnqXfm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_dummy_patchcore()\n",
        "#test_patchcore_real_data()"
      ],
      "metadata": {
        "id": "HUj2T7Lvkpk1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run patchcore"
      ],
      "metadata": {
        "id": "jgVPQqVkUG2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "import click\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "_DATASETS = {\"mvtec\": [\"mvtec\", \"MVTecDataset\"]}\n",
        "\n",
        "\n",
        "@click.group(chain=True)\n",
        "@click.argument(\"results_path\", type=str)\n",
        "@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n",
        "@click.option(\"--seed\", type=int, default=0, show_default=True)\n",
        "@click.option(\"--log_group\", type=str, default=\"group\")\n",
        "@click.option(\"--log_project\", type=str, default=\"project\")\n",
        "@click.option(\"--save_segmentation_images\", is_flag=True)\n",
        "@click.option(\"--save_patchcore_model\", is_flag=True)\n",
        "def main(**kwargs):\n",
        "    pass\n",
        "\n",
        "@main.result_callback()\n",
        "def run(\n",
        "    methods,\n",
        "    results_path,\n",
        "    gpu,\n",
        "    seed,\n",
        "    log_group,\n",
        "    log_project,\n",
        "    save_segmentation_images, \n",
        "    save_patchcore_model,\n",
        "):\n",
        "\n",
        "        methods = {key: item for (key, item) in methods}\n",
        "\n",
        "        run_save_path = create_storage_folder(\n",
        "            results_path, log_project, log_group, mode=\"iterate\"\n",
        "        )\n",
        "\n",
        "        list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n",
        "\n",
        "        device = set_torch_device(gpu)\n",
        "        # Device context here is specifically set and used later\n",
        "        # because there was GPU memory-bleeding which I could only fix with\n",
        "        # context managers.\n",
        "        device_context = (\n",
        "            torch.cuda.device(\"cuda:{}\".format(device.index))\n",
        "            if \"cuda\" in device.type.lower()\n",
        "            else contextlib.suppress()\n",
        "        )\n",
        "\n",
        "        result_collect = []\n",
        "\n",
        "        for dataloader_count, dataloaders in enumerate(list_of_dataloaders):\n",
        "            LOGGER.info(\n",
        "                \"Evaluating dataset [{}] ({}/{})...\".format(\n",
        "                    dataloaders[\"training\"].name,\n",
        "                    dataloader_count + 1,\n",
        "                    len(list_of_dataloaders),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            fix_seeds(seed, device)\n",
        "\n",
        "            dataset_name = dataloaders[\"training\"].name\n",
        "\n",
        "            with device_context:\n",
        "                torch.cuda.empty_cache()\n",
        "                imagesize = dataloaders[\"training\"].dataset.imagesize\n",
        "                sampler = methods[\"get_sampler\"](\n",
        "                    device,\n",
        "                )\n",
        "                PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n",
        "                if len(PatchCore_list) > 1:\n",
        "                    LOGGER.info(\n",
        "                        \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n",
        "                    )\n",
        "                for i, PatchCore in enumerate(PatchCore_list):\n",
        "                    torch.cuda.empty_cache()\n",
        "                    if PatchCore.backbone.seed is not None:\n",
        "                        fix_seeds(PatchCore.backbone.seed, device)\n",
        "                    LOGGER.info(\n",
        "                        \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n",
        "                    )\n",
        "                    torch.cuda.empty_cache()\n",
        "                    PatchCore.fit(dataloaders[\"training\"])\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                aggregator = {\"scores\": [], \"segmentations\": []}\n",
        "                for i, PatchCore in enumerate(PatchCore_list):\n",
        "                    torch.cuda.empty_cache()\n",
        "                    LOGGER.info(\n",
        "                        \"Embedding test data with models ({}/{})\".format(\n",
        "                            i + 1, len(PatchCore_list)\n",
        "                        )\n",
        "                    )\n",
        "                    scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n",
        "                        dataloaders[\"testing\"]\n",
        "                    )\n",
        "                    aggregator[\"scores\"].append(scores)\n",
        "                    aggregator[\"segmentations\"].append(segmentations)\n",
        "\n",
        "                scores = np.array(aggregator[\"scores\"])\n",
        "                min_scores = scores.min(axis=-1).reshape(-1, 1)\n",
        "                max_scores = scores.max(axis=-1).reshape(-1, 1)\n",
        "                scores = (scores - min_scores) / (max_scores - min_scores)\n",
        "                scores = np.mean(scores, axis=0)\n",
        "\n",
        "                segmentations = np.array(aggregator[\"segmentations\"])\n",
        "                min_scores = (\n",
        "                    segmentations.reshape(len(segmentations), -1)\n",
        "                    .min(axis=-1)\n",
        "                    .reshape(-1, 1, 1, 1)\n",
        "                )\n",
        "                max_scores = (\n",
        "                    segmentations.reshape(len(segmentations), -1)\n",
        "                    .max(axis=-1)\n",
        "                    .reshape(-1, 1, 1, 1)\n",
        "                )\n",
        "                segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n",
        "                segmentations = np.mean(segmentations, axis=0)\n",
        "\n",
        "                anomaly_labels = [\n",
        "                    x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                ]\n",
        "\n",
        "                # (Optional) Plot example images.\n",
        "                if save_segmentation_images:\n",
        "                    image_paths = [\n",
        "                        x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                    ]\n",
        "                    mask_paths = [\n",
        "                        x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                    ]\n",
        "\n",
        "                    def image_transform(image):\n",
        "                        dataloaders[\"testing\"].dataset.transform_std = [0.229, 0.224, 0.225]\n",
        "                        dataloaders[\"testing\"].dataset.transform_mean = [0.485, 0.456, 0.406]\n",
        "                        in_std = np.array(\n",
        "                            dataloaders[\"testing\"].dataset.transform_std\n",
        "                        ).reshape(-1, 1, 1)\n",
        "                        in_mean = np.array(\n",
        "                            dataloaders[\"testing\"].dataset.transform_mean\n",
        "                        ).reshape(-1, 1, 1)\n",
        "                        image = dataloaders[\"testing\"].dataset.transform_img(image)\n",
        "                        return np.clip(\n",
        "                            (image.numpy() * in_std + in_mean) * 255, 0, 255\n",
        "                        ).astype(np.uint8)\n",
        "\n",
        "                    def mask_transform(mask):\n",
        "                        return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n",
        "\n",
        "                    image_save_path = os.path.join(\n",
        "                        run_save_path, \"segmentation_images\", dataset_name\n",
        "                    )\n",
        "                    os.makedirs(image_save_path, exist_ok=True)\n",
        "                    plot_segmentation_images(\n",
        "                        image_save_path,\n",
        "                        image_paths,\n",
        "                        segmentations,\n",
        "                        scores,\n",
        "                        mask_paths,\n",
        "                        image_transform=image_transform,\n",
        "                        mask_transform=mask_transform,\n",
        "                    )\n",
        "\n",
        "                LOGGER.info(\"Computing evaluation metrics.\")\n",
        "                auroc = compute_imagewise_retrieval_metrics(\n",
        "                    scores, anomaly_labels\n",
        "                )[\"auroc\"]\n",
        "\n",
        "                # Compute PRO score & PW Auroc for all images\n",
        "                pixel_scores = compute_pixelwise_retrieval_metrics(\n",
        "                    segmentations, masks_gt\n",
        "                )\n",
        "                full_pixel_auroc = pixel_scores[\"auroc\"]\n",
        "\n",
        "                # Compute PRO score & PW Auroc only images with anomalies\n",
        "                sel_idxs = []\n",
        "                for i in range(len(masks_gt)):\n",
        "                    if np.sum(masks_gt[i]) > 0:\n",
        "                        sel_idxs.append(i)\n",
        "                pixel_scores = compute_pixelwise_retrieval_metrics(\n",
        "                    [segmentations[i] for i in sel_idxs],\n",
        "                    [masks_gt[i] for i in sel_idxs],\n",
        "                )\n",
        "                anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n",
        "\n",
        "                result_collect.append(\n",
        "                    {\n",
        "                        \"dataset_name\": dataset_name,\n",
        "                        \"instance_auroc\": auroc,\n",
        "                        \"full_pixel_auroc\": full_pixel_auroc,\n",
        "                        \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                for key, item in result_collect[-1].items():\n",
        "                    if key != \"dataset_name\":\n",
        "                        LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n",
        "\n",
        "                # (Optional) Store PatchCore model for later re-use.\n",
        "                # SAVE all patchcores only if mean_threshold is passed?\n",
        "                if save_patchcore_model:\n",
        "                    patchcore_save_path = os.path.join(\n",
        "                        run_save_path, \"models\", dataset_name\n",
        "                    )\n",
        "                    os.makedirs(patchcore_save_path, exist_ok=True)\n",
        "                    for i, PatchCore in enumerate(PatchCore_list):\n",
        "                        prepend = (\n",
        "                            \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n",
        "                            if len(PatchCore_list) > 1\n",
        "                            else \"\"\n",
        "                        )\n",
        "                        PatchCore.save_to_path(patchcore_save_path, prepend)\n",
        "\n",
        "            LOGGER.info(\"\\n\\n-----\\n\")\n",
        "\n",
        "        # Store all results and mean scores to a csv-file.\n",
        "        result_metric_names = list(result_collect[-1].keys())[1:]\n",
        "        result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n",
        "        result_scores = [list(results.values())[1:] for results in result_collect]\n",
        "        compute_and_store_final_results(\n",
        "            run_save_path,\n",
        "            result_scores,\n",
        "            column_names=result_metric_names,\n",
        "            row_names=result_dataset_names,\n",
        "        )\n",
        "        return 0\n",
        "    \n",
        "\n",
        "\n",
        "@main.command(\"patch_core\")\n",
        "# Pretraining-specific parameters.\n",
        "@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n",
        "@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n",
        "# Parameters for Glue-code (to merge different parts of the pipeline.\n",
        "@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)\n",
        "@click.option(\"--target_embed_dimension\", type=int, default=1024)\n",
        "@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n",
        "@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n",
        "# Nearest-Neighbour Anomaly Scorer parameters.\n",
        "@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n",
        "# Patch-parameters.\n",
        "@click.option(\"--patchsize\", type=int, default=3)\n",
        "@click.option(\"--patchscore\", type=str, default=\"max\")\n",
        "@click.option(\"--patchoverlap\", type=float, default=0.0)\n",
        "@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])\n",
        "# NN on GPU.\n",
        "@click.option(\"--faiss_on_gpu\", is_flag=True)\n",
        "@click.option(\"--faiss_num_workers\", type=int, default=8)\n",
        "def patch_core(\n",
        "    backbone_names,\n",
        "    layers_to_extract_from,\n",
        "    pretrain_embed_dimension,\n",
        "    target_embed_dimension,\n",
        "    preprocessing,\n",
        "    aggregation,\n",
        "    patchsize,\n",
        "    patchscore,\n",
        "    patchoverlap,\n",
        "    anomaly_scorer_num_nn,\n",
        "    patchsize_aggregate,\n",
        "    faiss_on_gpu,\n",
        "    faiss_num_workers,\n",
        "):\n",
        "    backbone_names = list(backbone_names)\n",
        "    if len(backbone_names) > 1:\n",
        "        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n",
        "        for layer in layers_to_extract_from:\n",
        "            idx = int(layer.split(\".\")[0])\n",
        "            layer = \".\".join(layer.split(\".\")[1:])\n",
        "            layers_to_extract_from_coll[idx].append(layer)\n",
        "    else:\n",
        "        layers_to_extract_from_coll = [layers_to_extract_from]\n",
        "\n",
        "    def get_patchcore(input_shape, sampler, device):\n",
        "        loaded_patchcores = []\n",
        "        for backbone_name, layers_to_extract_from in zip(\n",
        "            backbone_names, layers_to_extract_from_coll\n",
        "        ):\n",
        "            backbone_seed = None\n",
        "            if \".seed-\" in backbone_name:\n",
        "                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n",
        "                    backbone_name.split(\"-\")[-1]\n",
        "                )\n",
        "            backbone = load(backbone_name)\n",
        "            backbone.name, backbone.seed = backbone_name, backbone_seed\n",
        "\n",
        "            nn_method = FaissNN(faiss_on_gpu, faiss_num_workers)\n",
        "\n",
        "            patchcore_instance = PatchCore(device)\n",
        "            patchcore_instance.load(\n",
        "                backbone=backbone,\n",
        "                layers_to_extract_from=layers_to_extract_from,\n",
        "                device=device,\n",
        "                input_shape=input_shape,\n",
        "                pretrain_embed_dimension=pretrain_embed_dimension,\n",
        "                target_embed_dimension=target_embed_dimension,\n",
        "                patchsize=patchsize,\n",
        "                featuresampler=sampler,\n",
        "                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n",
        "                nn_method=nn_method,\n",
        "            )\n",
        "            loaded_patchcores.append(patchcore_instance)\n",
        "        return loaded_patchcores\n",
        "\n",
        "    return (\"get_patchcore\", get_patchcore)\n",
        "\n",
        "\n",
        "@main.command(\"sampler\")\n",
        "@click.argument(\"name\", type=str)\n",
        "@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\n",
        "def sampler(name, percentage):\n",
        "    def get_sampler(device):\n",
        "        if name == \"identity\":\n",
        "            return IdentitySampler()\n",
        "        elif name == \"greedy_coreset\":\n",
        "            return GreedyCoresetSampler(percentage, device)\n",
        "        elif name == \"approx_greedy_coreset\":\n",
        "            return ApproximateGreedyCoresetSampler(percentage, device)\n",
        "\n",
        "    return (\"get_sampler\", get_sampler)\n",
        "\n",
        "\n",
        "@main.command(\"dataset\")\n",
        "@click.argument(\"name\", type=str)\n",
        "@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n",
        "@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n",
        "@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n",
        "@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n",
        "@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n",
        "@click.option(\"--resize\", default=256, type=int, show_default=True)\n",
        "@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n",
        "@click.option(\"--augment\", is_flag=True)\n",
        "def dataset(\n",
        "    name,\n",
        "    data_path,\n",
        "    subdatasets,\n",
        "    train_val_split,\n",
        "    batch_size,\n",
        "    resize,\n",
        "    imagesize,\n",
        "    num_workers,\n",
        "    augment,\n",
        "):\n",
        "    dataset_info = _DATASETS[name]\n",
        "    #dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n",
        "\n",
        "    def get_dataloaders(seed):\n",
        "        dataloaders = []\n",
        "        for subdataset in subdatasets:\n",
        "\n",
        "            train_dataset = MVTecDataset(\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                train_val_split=train_val_split,\n",
        "                imagesize=imagesize,\n",
        "                split=DatasetSplit.TRAIN,\n",
        "                seed=seed,\n",
        "                augment=augment,\n",
        "            )\n",
        "\n",
        "            test_dataset = MVTecDataset(\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                imagesize=imagesize,\n",
        "                split=DatasetSplit.TEST,\n",
        "                seed=seed,\n",
        "            )\n",
        "\n",
        "            train_dataloader = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            test_dataloader = torch.utils.data.DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            train_dataloader.name = name\n",
        "            if subdataset is not None:\n",
        "                train_dataloader.name += \"_\" + subdataset\n",
        "\n",
        "            if train_val_split < 1:\n",
        "                val_dataset = MVTecDataset(\n",
        "                    data_path,\n",
        "                    classname=subdataset,\n",
        "                    resize=resize,\n",
        "                    train_val_split=train_val_split,\n",
        "                    imagesize=imagesize,\n",
        "                    split=DatasetSplit.VAL,\n",
        "                    seed=seed,\n",
        "                )\n",
        "\n",
        "                val_dataloader = torch.utils.data.DataLoader(\n",
        "                    val_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=False,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True,\n",
        "                )\n",
        "            else:\n",
        "                val_dataloader = None\n",
        "            dataloader_dict = {\n",
        "                \"training\": train_dataloader,\n",
        "                \"validation\": val_dataloader,\n",
        "                \"testing\": test_dataloader,\n",
        "            }\n",
        "\n",
        "            dataloaders.append(dataloader_dict)\n",
        "        return dataloaders\n",
        "\n",
        "    return (\"get_dataloaders\", get_dataloaders)\n",
        "\n",
        "\n",
        "#  \"[bottle, cable, capsule, carpet, grid, hazelnut, leather, metal_nut, pill, screw, tile, toothbrush, transistor, wood, zipper,]\",\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    argm= ['--gpu', '0', '--seed', '0',  '--save_segmentation_images', \\\n",
        "             '--log_group', 'IM224_WR50_L2-3_P01_D1024-1024_PS-3_AN-1_S0', '--log_project', 'MVTecAD_Results', 'drive/MyDrive/mvtec/', 'patch_core', '-b', 'wideresnet50', '-le', 'layer2', '-le', 'layer3', \\\n",
        "             '--pretrain_embed_dimension', '1024', '--target_embed_dimension', '1024', '--anomaly_scorer_num_nn', '1', '--patchsize', '3', 'sampler', '-p', '0.1', 'approx_greedy_coreset', \\\n",
        "             'dataset', '--resize', '256', '--imagesize', '224', '--subdatasets', 'zipper' , 'mvtec', 'drive/MyDrive/mvtec/']\n",
        "    \n",
        "    \n",
        "   # LOGGER.info(\"Command line arguments: {}\".format(\",\".join(argm)))\n",
        "\n",
        "    \n",
        "    main(argm)\n",
        " \n",
        " \n",
        "    \n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "UHGpWz3i_a-N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf956459-df53-4499-effd-40d10f0f5ea5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "Computing support features...:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Computing support features...:   1%|          | 1/120 [00:01<02:35,  1.31s/it]\u001b[A\n",
            "Computing support features...:   2%|▏         | 2/120 [00:01<01:28,  1.34it/s]\u001b[A\n",
            "Computing support features...:   2%|▎         | 3/120 [00:01<00:55,  2.13it/s]\u001b[A\n",
            "Computing support features...:   3%|▎         | 4/120 [00:01<00:38,  3.04it/s]\u001b[A\n",
            "Computing support features...:   4%|▍         | 5/120 [00:02<00:32,  3.51it/s]\u001b[A\n",
            "Computing support features...:   5%|▌         | 6/120 [00:02<00:28,  4.02it/s]\u001b[A\n",
            "Computing support features...:   6%|▌         | 7/120 [00:02<00:30,  3.69it/s]\u001b[A\n",
            "Computing support features...:   7%|▋         | 8/120 [00:02<00:28,  3.96it/s]\u001b[A\n",
            "Computing support features...:   8%|▊         | 9/120 [00:03<00:29,  3.74it/s]\u001b[A\n",
            "Computing support features...:   8%|▊         | 10/120 [00:03<00:27,  4.06it/s]\u001b[A\n",
            "Computing support features...:   9%|▉         | 11/120 [00:03<00:24,  4.40it/s]\u001b[A\n",
            "Computing support features...:  10%|█         | 12/120 [00:03<00:24,  4.44it/s]\u001b[A\n",
            "Computing support features...:  11%|█         | 13/120 [00:03<00:24,  4.44it/s]\u001b[A\n",
            "Computing support features...:  12%|█▏        | 14/120 [00:04<00:21,  5.02it/s]\u001b[A\n",
            "Computing support features...:  12%|█▎        | 15/120 [00:04<00:22,  4.74it/s]\u001b[A\n",
            "Computing support features...:  13%|█▎        | 16/120 [00:04<00:20,  5.06it/s]\u001b[A\n",
            "Computing support features...:  14%|█▍        | 17/120 [00:04<00:19,  5.20it/s]\u001b[A\n",
            "Computing support features...:  15%|█▌        | 18/120 [00:04<00:19,  5.28it/s]\u001b[A\n",
            "Computing support features...:  16%|█▌        | 19/120 [00:05<00:18,  5.58it/s]\u001b[A\n",
            "Computing support features...:  17%|█▋        | 20/120 [00:05<00:16,  5.98it/s]\u001b[A\n",
            "Computing support features...:  18%|█▊        | 21/120 [00:05<00:18,  5.42it/s]\u001b[A\n",
            "Computing support features...:  18%|█▊        | 22/120 [00:05<00:16,  5.91it/s]\u001b[A\n",
            "Computing support features...:  19%|█▉        | 23/120 [00:05<00:16,  5.74it/s]\u001b[A\n",
            "Computing support features...:  20%|██        | 24/120 [00:05<00:17,  5.38it/s]\u001b[A\n",
            "Computing support features...:  21%|██        | 25/120 [00:06<00:16,  5.84it/s]\u001b[A\n",
            "Computing support features...:  22%|██▏       | 26/120 [00:06<00:16,  5.55it/s]\u001b[A\n",
            "Computing support features...:  22%|██▎       | 27/120 [00:06<00:17,  5.43it/s]\u001b[A\n",
            "Computing support features...:  23%|██▎       | 28/120 [00:06<00:16,  5.68it/s]\u001b[A\n",
            "Computing support features...:  24%|██▍       | 29/120 [00:06<00:15,  5.97it/s]\u001b[A\n",
            "Computing support features...:  25%|██▌       | 30/120 [00:06<00:14,  6.42it/s]\u001b[A\n",
            "Computing support features...:  26%|██▌       | 31/120 [00:07<00:14,  6.34it/s]\u001b[A\n",
            "Computing support features...:  27%|██▋       | 32/120 [00:07<00:12,  6.83it/s]\u001b[A\n",
            "Computing support features...:  28%|██▊       | 33/120 [00:07<00:12,  6.89it/s]\u001b[A\n",
            "Computing support features...:  28%|██▊       | 34/120 [00:07<00:11,  7.25it/s]\u001b[A\n",
            "Computing support features...:  29%|██▉       | 35/120 [00:07<00:11,  7.52it/s]\u001b[A\n",
            "Computing support features...:  30%|███       | 36/120 [00:07<00:10,  7.96it/s]\u001b[A\n",
            "Computing support features...:  31%|███       | 37/120 [00:07<00:10,  7.62it/s]\u001b[A\n",
            "Computing support features...:  32%|███▏      | 38/120 [00:07<00:10,  7.69it/s]\u001b[A\n",
            "Computing support features...:  32%|███▎      | 39/120 [00:08<00:10,  8.03it/s]\u001b[A\n",
            "Computing support features...:  33%|███▎      | 40/120 [00:08<00:09,  8.37it/s]\u001b[A\n",
            "Computing support features...:  34%|███▍      | 41/120 [00:08<00:10,  7.66it/s]\u001b[A\n",
            "Computing support features...:  35%|███▌      | 42/120 [00:08<00:09,  7.94it/s]\u001b[A\n",
            "Computing support features...:  36%|███▌      | 43/120 [00:08<00:09,  8.15it/s]\u001b[A\n",
            "Computing support features...:  37%|███▋      | 44/120 [00:08<00:09,  7.82it/s]\u001b[A\n",
            "Computing support features...:  38%|███▊      | 45/120 [00:08<00:09,  8.08it/s]\u001b[A\n",
            "Computing support features...:  38%|███▊      | 46/120 [00:08<00:10,  7.40it/s]\u001b[A\n",
            "Computing support features...:  39%|███▉      | 47/120 [00:09<00:09,  7.78it/s]\u001b[A\n",
            "Computing support features...:  40%|████      | 48/120 [00:09<00:08,  8.14it/s]\u001b[A\n",
            "Computing support features...:  41%|████      | 49/120 [00:09<00:09,  7.82it/s]\u001b[A\n",
            "Computing support features...:  42%|████▏     | 50/120 [00:09<00:08,  8.18it/s]\u001b[A\n",
            "Computing support features...:  42%|████▎     | 51/120 [00:09<00:08,  8.46it/s]\u001b[A\n",
            "Computing support features...:  43%|████▎     | 52/120 [00:09<00:07,  8.62it/s]\u001b[A\n",
            "Computing support features...:  44%|████▍     | 53/120 [00:09<00:08,  8.33it/s]\u001b[A\n",
            "Computing support features...:  45%|████▌     | 54/120 [00:09<00:08,  8.04it/s]\u001b[A\n",
            "Computing support features...:  46%|████▌     | 55/120 [00:10<00:08,  7.71it/s]\u001b[A\n",
            "Computing support features...:  47%|████▋     | 56/120 [00:10<00:08,  7.93it/s]\u001b[A\n",
            "Computing support features...:  48%|████▊     | 57/120 [00:10<00:07,  7.93it/s]\u001b[A\n",
            "Computing support features...:  48%|████▊     | 58/120 [00:10<00:07,  8.27it/s]\u001b[A\n",
            "Computing support features...:  49%|████▉     | 59/120 [00:10<00:07,  8.51it/s]\u001b[A\n",
            "Computing support features...:  50%|█████     | 60/120 [00:10<00:06,  8.76it/s]\u001b[A\n",
            "Computing support features...:  51%|█████     | 61/120 [00:10<00:06,  8.54it/s]\u001b[A\n",
            "Computing support features...:  52%|█████▏    | 62/120 [00:10<00:06,  8.72it/s]\u001b[A\n",
            "Computing support features...:  52%|█████▎    | 63/120 [00:10<00:06,  8.79it/s]\u001b[A\n",
            "Computing support features...:  53%|█████▎    | 64/120 [00:11<00:07,  7.40it/s]\u001b[A\n",
            "Computing support features...:  54%|█████▍    | 65/120 [00:11<00:07,  7.83it/s]\u001b[A\n",
            "Computing support features...:  55%|█████▌    | 66/120 [00:11<00:06,  8.12it/s]\u001b[A\n",
            "Computing support features...:  56%|█████▌    | 67/120 [00:11<00:06,  8.29it/s]\u001b[A\n",
            "Computing support features...:  57%|█████▋    | 68/120 [00:11<00:06,  7.49it/s]\u001b[A\n",
            "Computing support features...:  57%|█████▊    | 69/120 [00:11<00:06,  7.64it/s]\u001b[A\n",
            "Computing support features...:  58%|█████▊    | 70/120 [00:11<00:06,  7.96it/s]\u001b[A\n",
            "Computing support features...:  59%|█████▉    | 71/120 [00:12<00:06,  7.87it/s]\u001b[A\n",
            "Computing support features...:  60%|██████    | 72/120 [00:12<00:06,  7.74it/s]\u001b[A\n",
            "Computing support features...:  61%|██████    | 73/120 [00:12<00:06,  7.15it/s]\u001b[A\n",
            "Computing support features...:  62%|██████▏   | 74/120 [00:12<00:08,  5.60it/s]\u001b[A\n",
            "Computing support features...:  62%|██████▎   | 75/120 [00:12<00:08,  5.58it/s]\u001b[A\n",
            "Computing support features...:  63%|██████▎   | 76/120 [00:12<00:07,  5.69it/s]\u001b[A\n",
            "Computing support features...:  64%|██████▍   | 77/120 [00:13<00:09,  4.43it/s]\u001b[A\n",
            "Computing support features...:  65%|██████▌   | 78/120 [00:13<00:08,  5.11it/s]\u001b[A\n",
            "Computing support features...:  66%|██████▌   | 79/120 [00:13<00:07,  5.77it/s]\u001b[A\n",
            "Computing support features...:  67%|██████▋   | 80/120 [00:13<00:06,  6.48it/s]\u001b[A\n",
            "Computing support features...:  68%|██████▊   | 81/120 [00:13<00:05,  6.90it/s]\u001b[A\n",
            "Computing support features...:  68%|██████▊   | 82/120 [00:13<00:05,  7.46it/s]\u001b[A\n",
            "Computing support features...:  69%|██████▉   | 83/120 [00:13<00:04,  7.69it/s]\u001b[A\n",
            "Computing support features...:  70%|███████   | 84/120 [00:14<00:04,  7.23it/s]\u001b[A\n",
            "Computing support features...:  71%|███████   | 85/120 [00:14<00:04,  7.69it/s]\u001b[A\n",
            "Computing support features...:  72%|███████▏  | 86/120 [00:14<00:04,  8.03it/s]\u001b[A\n",
            "Computing support features...:  72%|███████▎  | 87/120 [00:14<00:03,  8.40it/s]\u001b[A\n",
            "Computing support features...:  73%|███████▎  | 88/120 [00:14<00:03,  8.43it/s]\u001b[A\n",
            "Computing support features...:  74%|███████▍  | 89/120 [00:14<00:03,  8.46it/s]\u001b[A\n",
            "Computing support features...:  75%|███████▌  | 90/120 [00:14<00:03,  8.53it/s]\u001b[A\n",
            "Computing support features...:  76%|███████▌  | 91/120 [00:14<00:03,  8.74it/s]\u001b[A\n",
            "Computing support features...:  77%|███████▋  | 92/120 [00:15<00:03,  8.77it/s]\u001b[A\n",
            "Computing support features...:  78%|███████▊  | 93/120 [00:15<00:03,  8.69it/s]\u001b[A\n",
            "Computing support features...:  78%|███████▊  | 94/120 [00:15<00:02,  8.81it/s]\u001b[A\n",
            "Computing support features...:  79%|███████▉  | 95/120 [00:15<00:02,  8.89it/s]\u001b[A\n",
            "Computing support features...:  80%|████████  | 96/120 [00:15<00:02,  9.05it/s]\u001b[A\n",
            "Computing support features...:  81%|████████  | 97/120 [00:15<00:02,  9.27it/s]\u001b[A\n",
            "Computing support features...:  82%|████████▏ | 98/120 [00:15<00:02,  9.07it/s]\u001b[A\n",
            "Computing support features...:  82%|████████▎ | 99/120 [00:15<00:02,  9.04it/s]\u001b[A\n",
            "Computing support features...:  83%|████████▎ | 100/120 [00:15<00:02,  9.05it/s]\u001b[A\n",
            "Computing support features...:  84%|████████▍ | 101/120 [00:16<00:02,  9.18it/s]\u001b[A\n",
            "Computing support features...:  85%|████████▌ | 102/120 [00:16<00:01,  9.37it/s]\u001b[A\n",
            "Computing support features...:  86%|████████▌ | 103/120 [00:16<00:01,  9.39it/s]\u001b[A\n",
            "Computing support features...:  88%|████████▊ | 105/120 [00:16<00:01, 10.38it/s]\u001b[A\n",
            "Computing support features...:  89%|████████▉ | 107/120 [00:16<00:01, 11.17it/s]\u001b[A\n",
            "Computing support features...:  91%|█████████ | 109/120 [00:16<00:00, 12.26it/s]\u001b[A\n",
            "Computing support features...:  92%|█████████▎| 111/120 [00:16<00:00, 12.49it/s]\u001b[A\n",
            "Computing support features...:  94%|█████████▍| 113/120 [00:17<00:00, 12.34it/s]\u001b[A\n",
            "Computing support features...:  96%|█████████▌| 115/120 [00:17<00:00, 12.32it/s]\u001b[A\n",
            "Computing support features...:  98%|█████████▊| 117/120 [00:17<00:00, 12.42it/s]\u001b[A\n",
            "Computing support features...:  99%|█████████▉| 119/120 [00:17<00:00, 12.49it/s]\u001b[A\n",
            "Subsampling...: 100%|██████████| 18816/18816 [00:27<00:00, 676.54it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load and evaluate"
      ],
      "metadata": {
        "id": "E5fPncRyvz0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import gc\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import click\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "_DATASETS = {\"mvtec\": [\"mvtec\", \"MVTecDataset\"]}\n",
        "\n",
        "\n",
        "@click.group(chain=True)\n",
        "@click.argument(\"results_path\", type=str)\n",
        "@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n",
        "@click.option(\"--seed\", type=int, default=0, show_default=True)\n",
        "@click.option(\"--save_segmentation_images\", is_flag=True)\n",
        "def main(**kwargs):\n",
        "    pass\n",
        "\n",
        "\n",
        "@main.result_callback()\n",
        "def run(methods, results_path, gpu, seed, save_segmentation_images):\n",
        "    methods = {key: item for (key, item) in methods}\n",
        "\n",
        "    os.makedirs(results_path, exist_ok=True)\n",
        "\n",
        "    device = set_torch_device(gpu)\n",
        "    # Device context here is specifically set and used later\n",
        "    # because there was GPU memory-bleeding which I could only fix with\n",
        "    # context managers.\n",
        "    device_context = (\n",
        "        torch.cuda.device(\"cuda:{}\".format(device.index))\n",
        "        if \"cuda\" in device.type.lower()\n",
        "        else contextlib.suppress()\n",
        "    )\n",
        "\n",
        "    result_collect = []\n",
        "\n",
        "    dataloader_iter, n_dataloaders = methods[\"get_dataloaders_iter\"]\n",
        "    dataloader_iter = dataloader_iter(seed)\n",
        "    patchcore_iter, n_patchcores = methods[\"get_patchcore_iter\"]\n",
        "    patchcore_iter = patchcore_iter(device)\n",
        "    if not (n_dataloaders == n_patchcores or n_patchcores == 1):\n",
        "        raise ValueError(\n",
        "            \"Please ensure that #PatchCores == #Datasets or #PatchCores == 1!\"\n",
        "        )\n",
        "\n",
        "    for dataloader_count, dataloaders in enumerate(dataloader_iter):\n",
        "        LOGGER.info(\n",
        "            \"Evaluating dataset [{}] ({}/{})...\".format(\n",
        "                dataloaders[\"testing\"].name, dataloader_count + 1, n_dataloaders\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fix_seeds(seed, device)\n",
        "\n",
        "        dataset_name = dataloaders[\"testing\"].name\n",
        "\n",
        "        with device_context:\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            if dataloader_count < n_patchcores:\n",
        "                PatchCore_list = next(patchcore_iter)\n",
        "\n",
        "            aggregator = {\"scores\": [], \"segmentations\": []}\n",
        "            for i, PatchCore in enumerate(PatchCore_list):\n",
        "                torch.cuda.empty_cache()\n",
        "                LOGGER.info(\n",
        "                    \"Embedding test data with models ({}/{})\".format(\n",
        "                        i + 1, len(PatchCore_list)\n",
        "                    )\n",
        "                )\n",
        "                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n",
        "                    dataloaders[\"testing\"]\n",
        "                )\n",
        "                aggregator[\"scores\"].append(scores)\n",
        "                aggregator[\"segmentations\"].append(segmentations)\n",
        "\n",
        "            scores = np.array(aggregator[\"scores\"])\n",
        "            min_scores = scores.min(axis=-1).reshape(-1, 1)\n",
        "            max_scores = scores.max(axis=-1).reshape(-1, 1)\n",
        "            scores = (scores - min_scores) / (max_scores - min_scores)\n",
        "            scores = np.mean(scores, axis=0)\n",
        "\n",
        "            segmentations = np.array(aggregator[\"segmentations\"])\n",
        "            min_scores = (\n",
        "                segmentations.reshape(len(segmentations), -1)\n",
        "                .min(axis=-1)\n",
        "                .reshape(-1, 1, 1, 1)\n",
        "            )\n",
        "            max_scores = (\n",
        "                segmentations.reshape(len(segmentations), -1)\n",
        "                .max(axis=-1)\n",
        "                .reshape(-1, 1, 1, 1)\n",
        "            )\n",
        "            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n",
        "            segmentations = np.mean(segmentations, axis=0)\n",
        "\n",
        "            anomaly_labels = [\n",
        "                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "            ]\n",
        "\n",
        "            # Plot Example Images.\n",
        "            if save_segmentation_images:\n",
        "                image_paths = [\n",
        "                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                ]\n",
        "                mask_paths = [\n",
        "                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                ]\n",
        "\n",
        "                def image_transform(image):\n",
        "                    in_std = np.array(\n",
        "                        dataloaders[\"testing\"].dataset.transform_std\n",
        "                    ).reshape(-1, 1, 1)\n",
        "                    in_mean = np.array(\n",
        "                        dataloaders[\"testing\"].dataset.transform_mean\n",
        "                    ).reshape(-1, 1, 1)\n",
        "                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n",
        "                    return np.clip(\n",
        "                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n",
        "                    ).astype(np.uint8)\n",
        "\n",
        "                def mask_transform(mask):\n",
        "                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n",
        "\n",
        "                plot_segmentation_images(\n",
        "                    results_path,\n",
        "                    image_paths,\n",
        "                    segmentations,\n",
        "                    scores,\n",
        "                    mask_paths,\n",
        "                    image_transform=image_transform,\n",
        "                    mask_transform=mask_transform,\n",
        "                )\n",
        "\n",
        "            LOGGER.info(\"Computing evaluation metrics.\")\n",
        "            # Compute Image-level AUROC scores for all images.\n",
        "            auroc = compute_imagewise_retrieval_metrics(\n",
        "                scores, anomaly_labels\n",
        "            )[\"auroc\"]\n",
        "\n",
        "            # Compute PRO score & PW Auroc for all images\n",
        "            pixel_scores = compute_pixelwise_retrieval_metrics(\n",
        "                segmentations, masks_gt\n",
        "            )\n",
        "            full_pixel_auroc = pixel_scores[\"auroc\"]\n",
        "\n",
        "            # Compute PRO score & PW Auroc only for images with anomalies\n",
        "            sel_idxs = []\n",
        "            for i in range(len(masks_gt)):\n",
        "                if np.sum(masks_gt[i]) > 0:\n",
        "                    sel_idxs.append(i)\n",
        "            pixel_scores = compute_pixelwise_retrieval_metrics(\n",
        "                [segmentations[i] for i in sel_idxs], [masks_gt[i] for i in sel_idxs]\n",
        "            )\n",
        "            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n",
        "\n",
        "            result_collect.append(\n",
        "                {\n",
        "                    \"dataset_name\": dataset_name,\n",
        "                    \"instance_auroc\": auroc,\n",
        "                    \"full_pixel_auroc\": full_pixel_auroc,\n",
        "                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            for key, item in result_collect[-1].items():\n",
        "                if key != \"dataset_name\":\n",
        "                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n",
        "\n",
        "            del PatchCore_list\n",
        "            gc.collect()\n",
        "\n",
        "        LOGGER.info(\"\\n\\n-----\\n\")\n",
        "\n",
        "    result_metric_names = list(result_collect[-1].keys())[1:]\n",
        "    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n",
        "    result_scores = [list(results.values())[1:] for results in result_collect]\n",
        "    compute_and_store_final_results(\n",
        "        results_path,\n",
        "        result_scores,\n",
        "        column_names=result_metric_names,\n",
        "        row_names=result_dataset_names,\n",
        "    )\n",
        "\n",
        "\n",
        "@main.command(\"patch_core_loader\")\n",
        "# Pretraining-specific parameters.\n",
        "@click.option(\"--patch_core_paths\", \"-p\", type=str, multiple=True, default=[])\n",
        "# NN on GPU.\n",
        "@click.option(\"--faiss_on_gpu\", is_flag=True)\n",
        "@click.option(\"--faiss_num_workers\", type=int, default=8)\n",
        "def patch_core_loader(patch_core_paths, faiss_on_gpu, faiss_num_workers):\n",
        "    def get_patchcore_iter(device):\n",
        "        for patch_core_path in patch_core_paths:\n",
        "            loaded_patchcores = []\n",
        "            gc.collect()\n",
        "            n_patchcores = len(\n",
        "                [x for x in os.listdir(patch_core_path) if \".faiss\" in x]\n",
        "            )\n",
        "            if n_patchcores == 1:\n",
        "                nn_method = FaissNN(faiss_on_gpu, faiss_num_workers)\n",
        "                patchcore_instance = PatchCore(device)\n",
        "                patchcore_instance.load_from_path(\n",
        "                    load_path=patch_core_path, device=device, nn_method=nn_method\n",
        "                )\n",
        "                loaded_patchcores.append(patchcore_instance)\n",
        "            else:\n",
        "                for i in range(n_patchcores):\n",
        "                    nn_method = FaissNN(\n",
        "                        faiss_on_gpu, faiss_num_workers\n",
        "                    )\n",
        "                    patchcore_instance = PatchCore(device)\n",
        "                    patchcore_instance.load_from_path(\n",
        "                        load_path=patch_core_path,\n",
        "                        device=device,\n",
        "                        nn_method=nn_method,\n",
        "                        prepend=\"Ensemble-{}-{}_\".format(i + 1, n_patchcores),\n",
        "                    )\n",
        "                    loaded_patchcores.append(patchcore_instance)\n",
        "\n",
        "            yield loaded_patchcores\n",
        "\n",
        "    return (\"get_patchcore_iter\", [get_patchcore_iter, len(patch_core_paths)])\n",
        "\n",
        "\n",
        "@main.command(\"dataset\")\n",
        "@click.argument(\"name\", type=str)\n",
        "@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n",
        "@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n",
        "@click.option(\"--batch_size\", default=1, type=int, show_default=True)\n",
        "@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n",
        "@click.option(\"--resize\", default=256, type=int, show_default=True)\n",
        "@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n",
        "@click.option(\"--augment\", is_flag=True)\n",
        "def dataset(\n",
        "    name, data_path, subdatasets, batch_size, resize, imagesize, num_workers, augment\n",
        "):\n",
        "    dataset_info = _DATASETS[name]\n",
        "    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n",
        "\n",
        "    def get_dataloaders_iter(seed):\n",
        "        for subdataset in subdatasets:\n",
        "            test_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                imagesize=imagesize,\n",
        "                split=dataset_library.DatasetSplit.TEST,\n",
        "                seed=seed,\n",
        "            )\n",
        "\n",
        "            test_dataloader = torch.utils.data.DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            test_dataloader.name = name\n",
        "            if subdataset is not None:\n",
        "                test_dataloader.name += \"_\" + subdataset\n",
        "\n",
        "            dataloader_dict = {\"testing\": test_dataloader}\n",
        "\n",
        "            yield dataloader_dict\n",
        "\n",
        "    return (\"get_dataloaders_iter\", [get_dataloaders_iter, len(subdatasets)])\n",
        "\n",
        "\n",
        "\"\"\"if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n",
        "    main()\"\"\"\n"
      ],
      "metadata": {
        "id": "DGIZQJTCqXiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}